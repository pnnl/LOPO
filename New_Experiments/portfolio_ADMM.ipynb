{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLearning to solve parametric Quadratic Programming\\nportfolio optimization\\nproblems using Neuromancer.\\n\\nProblem formulation:\\n    minimize    - p^T x + lambda x^T Q x\\n    subject to       1^T x = 1\\n                      x >= 0\\n\\nWhere p is interpreted as a vector of asset returns, and Q represents\\nthe covariance between assets, which forms a penalty on overall\\ncovariance (risk) weighted by lambda.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Learning to solve parametric Quadratic Programming\n",
    "portfolio optimization\n",
    "problems using Neuromancer.\n",
    "\n",
    "Problem formulation:\n",
    "    minimize    - p^T x + lambda x^T Q x\n",
    "    subject to       1^T x = 1\n",
    "                      x >= 0\n",
    "\n",
    "Where p is interpreted as a vector of asset returns, and Q represents\n",
    "the covariance between assets, which forms a penalty on overall\n",
    "covariance (risk) weighted by lambda.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import neuromancer.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as patheffects\n",
    "\n",
    "from neuromancer.trainer import Trainer\n",
    "from neuromancer.problem import Problem\n",
    "from neuromancer.constraint import variable\n",
    "from neuromancer.dataset import DictDataset\n",
    "from neuromancer.loss import PenaltyLoss\n",
    "from neuromancer.modules import blocks\n",
    "from neuromancer.system import Node\n",
    "\n",
    "from portfolio_utils import gen_portfolio_lto_data, cvx_qp\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch.cvxpylayer import CvxpyLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset . . . \n",
      "Training set generated \n",
      "Validation set generated \n",
      "Test set generated \n",
      "p_train.dtype\n",
      "float64\n",
      "epoch: 0  train_loss: 115.43560791015625\n",
      "epoch: 1  train_loss: 91.48567962646484\n",
      "epoch: 2  train_loss: 66.46897888183594\n",
      "epoch: 3  train_loss: 38.81877136230469\n",
      "epoch: 4  train_loss: 13.978739738464355\n",
      "epoch: 5  train_loss: 22.993690490722656\n",
      "epoch: 6  train_loss: 23.581710815429688\n",
      "epoch: 7  train_loss: 12.298029899597168\n",
      "epoch: 8  train_loss: 8.137669563293457\n",
      "epoch: 9  train_loss: 10.1363525390625\n",
      "epoch: 10  train_loss: 5.584509372711182\n",
      "epoch: 11  train_loss: 6.087329864501953\n",
      "epoch: 12  train_loss: 4.547824382781982\n",
      "epoch: 13  train_loss: 6.356706142425537\n",
      "epoch: 14  train_loss: 4.15821647644043\n",
      "epoch: 15  train_loss: 6.28376579284668\n",
      "epoch: 16  train_loss: 4.0565056800842285\n",
      "epoch: 17  train_loss: 5.9060444831848145\n",
      "epoch: 18  train_loss: 3.849273443222046\n",
      "epoch: 19  train_loss: 5.424860000610352\n",
      "epoch: 20  train_loss: 3.6128768920898438\n",
      "epoch: 21  train_loss: 5.542415618896484\n",
      "epoch: 22  train_loss: 3.1963016986846924\n",
      "epoch: 23  train_loss: 2.9436709880828857\n",
      "epoch: 24  train_loss: 2.88000226020813\n",
      "epoch: 25  train_loss: 2.5711467266082764\n",
      "epoch: 26  train_loss: 2.4802091121673584\n",
      "epoch: 27  train_loss: 2.2399775981903076\n",
      "epoch: 28  train_loss: 2.099682569503784\n",
      "epoch: 29  train_loss: 1.9811744689941406\n",
      "epoch: 30  train_loss: 2.08109450340271\n",
      "epoch: 31  train_loss: 2.398083448410034\n",
      "epoch: 32  train_loss: 2.832460641860962\n",
      "epoch: 33  train_loss: 3.62738299369812\n",
      "epoch: 34  train_loss: 2.4740617275238037\n",
      "epoch: 35  train_loss: 2.4210140705108643\n",
      "epoch: 36  train_loss: 2.022963523864746\n",
      "epoch: 37  train_loss: 2.277667760848999\n",
      "epoch: 38  train_loss: 1.9851970672607422\n",
      "epoch: 39  train_loss: 2.1609914302825928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/king339/Projects/DAIDIST/neuromancer/src/neuromancer/constraint.py:160: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([100])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.l1_loss(left, right)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40  train_loss: 1.9939700365066528\n",
      "epoch: 41  train_loss: 2.068026542663574\n",
      "epoch: 42  train_loss: 1.9881242513656616\n",
      "epoch: 43  train_loss: 2.0224008560180664\n",
      "epoch: 44  train_loss: 1.959702968597412\n",
      "epoch: 45  train_loss: 1.9634078741073608\n",
      "epoch: 46  train_loss: 1.9549232721328735\n",
      "epoch: 47  train_loss: 1.9156936407089233\n",
      "epoch: 48  train_loss: 1.9825035333633423\n",
      "epoch: 49  train_loss: 1.8121739625930786\n",
      "epoch: 50  train_loss: 1.976203441619873\n",
      "epoch: 51  train_loss: 1.8023481369018555\n",
      "epoch: 52  train_loss: 2.0012941360473633\n",
      "epoch: 53  train_loss: 1.791285514831543\n",
      "epoch: 54  train_loss: 1.9869118928909302\n",
      "epoch: 55  train_loss: 1.4828788042068481\n",
      "epoch: 56  train_loss: 1.6011708974838257\n",
      "epoch: 57  train_loss: 1.3287853002548218\n",
      "epoch: 58  train_loss: 1.8671702146530151\n",
      "epoch: 59  train_loss: 2.0582804679870605\n",
      "epoch: 60  train_loss: 1.26935875415802\n",
      "epoch: 61  train_loss: 1.428207278251648\n",
      "epoch: 62  train_loss: 1.3038363456726074\n",
      "epoch: 63  train_loss: 1.4414504766464233\n",
      "epoch: 64  train_loss: 1.3009527921676636\n",
      "epoch: 65  train_loss: 1.2940057516098022\n",
      "epoch: 66  train_loss: 1.3693040609359741\n",
      "epoch: 67  train_loss: 1.7270474433898926\n",
      "epoch: 68  train_loss: 1.695992112159729\n",
      "epoch: 69  train_loss: 1.0769546031951904\n",
      "epoch: 70  train_loss: 1.3420330286026\n",
      "epoch: 71  train_loss: 1.0911394357681274\n",
      "epoch: 72  train_loss: 1.438327670097351\n",
      "epoch: 73  train_loss: 2.478100538253784\n",
      "epoch: 74  train_loss: 2.102320432662964\n",
      "epoch: 75  train_loss: 1.9797874689102173\n",
      "epoch: 76  train_loss: 1.4114454984664917\n",
      "epoch: 77  train_loss: 0.9869216084480286\n",
      "epoch: 78  train_loss: 1.2475292682647705\n",
      "epoch: 79  train_loss: 1.018520712852478\n",
      "epoch: 80  train_loss: 1.3312798738479614\n",
      "epoch: 81  train_loss: 2.4881749153137207\n",
      "epoch: 82  train_loss: 1.9217182397842407\n",
      "epoch: 83  train_loss: 1.8329063653945923\n",
      "epoch: 84  train_loss: 1.627982497215271\n",
      "epoch: 85  train_loss: 2.6361758708953857\n",
      "epoch: 86  train_loss: 1.6301289796829224\n",
      "epoch: 87  train_loss: 1.5733418464660645\n",
      "epoch: 88  train_loss: 2.446465253829956\n",
      "epoch: 89  train_loss: 1.689645767211914\n",
      "epoch: 90  train_loss: 1.8780254125595093\n",
      "epoch: 91  train_loss: 1.82831609249115\n",
      "epoch: 92  train_loss: 1.6818817853927612\n",
      "epoch: 93  train_loss: 2.2810184955596924\n",
      "epoch: 94  train_loss: 1.7122946977615356\n",
      "epoch: 95  train_loss: 1.6398277282714844\n",
      "epoch: 96  train_loss: 1.9303659200668335\n",
      "epoch: 97  train_loss: 2.085129976272583\n",
      "epoch: 98  train_loss: 1.6577345132827759\n",
      "epoch: 99  train_loss: 1.595929741859436\n",
      "epoch: 100  train_loss: 2.086824417114258\n",
      "epoch: 101  train_loss: 1.7473801374435425\n",
      "epoch: 102  train_loss: 1.8151332139968872\n",
      "epoch: 103  train_loss: 1.7276487350463867\n",
      "epoch: 104  train_loss: 1.4021774530410767\n",
      "epoch: 105  train_loss: 2.522887945175171\n",
      "epoch: 106  train_loss: 1.4306875467300415\n",
      "epoch: 107  train_loss: 0.9165007472038269\n",
      "epoch: 108  train_loss: 0.9918515682220459\n",
      "epoch: 109  train_loss: 0.7809774279594421\n",
      "epoch: 110  train_loss: 1.0878506898880005\n",
      "epoch: 111  train_loss: 2.1880364418029785\n",
      "epoch: 112  train_loss: 1.7282781600952148\n",
      "epoch: 113  train_loss: 1.6212767362594604\n",
      "epoch: 114  train_loss: 1.2203850746154785\n",
      "epoch: 115  train_loss: 0.7920475602149963\n",
      "epoch: 116  train_loss: 0.9572365283966064\n",
      "epoch: 117  train_loss: 0.8045527338981628\n",
      "epoch: 118  train_loss: 0.9244379997253418\n",
      "epoch: 119  train_loss: 0.8231964111328125\n",
      "epoch: 120  train_loss: 0.8595417141914368\n",
      "epoch: 121  train_loss: 0.849606454372406\n",
      "epoch: 122  train_loss: 0.8154494762420654\n",
      "epoch: 123  train_loss: 0.7250173687934875\n",
      "epoch: 124  train_loss: 1.0583759546279907\n",
      "epoch: 125  train_loss: 1.781324028968811\n",
      "epoch: 126  train_loss: 1.8936840295791626\n",
      "epoch: 127  train_loss: 1.8342900276184082\n",
      "epoch: 128  train_loss: 3.4998066425323486\n",
      "epoch: 129  train_loss: 2.107994318008423\n",
      "epoch: 130  train_loss: 1.280991792678833\n",
      "epoch: 131  train_loss: 1.706404685974121\n",
      "epoch: 132  train_loss: 1.7973092794418335\n",
      "epoch: 133  train_loss: 1.6907235383987427\n",
      "epoch: 134  train_loss: 0.9736215472221375\n",
      "epoch: 135  train_loss: 0.6377108097076416\n",
      "epoch: 136  train_loss: 1.0276628732681274\n",
      "epoch: 137  train_loss: 1.9470089673995972\n",
      "epoch: 138  train_loss: 1.6522754430770874\n",
      "epoch: 139  train_loss: 1.5788522958755493\n",
      "epoch: 140  train_loss: 1.074269413948059\n",
      "epoch: 141  train_loss: 0.706760585308075\n",
      "epoch: 142  train_loss: 0.7858104109764099\n",
      "epoch: 143  train_loss: 0.9621790051460266\n",
      "epoch: 144  train_loss: 1.5385112762451172\n",
      "epoch: 145  train_loss: 1.5084184408187866\n",
      "epoch: 146  train_loss: 1.7883473634719849\n",
      "epoch: 147  train_loss: 1.5346574783325195\n",
      "epoch: 148  train_loss: 1.6077576875686646\n",
      "epoch: 149  train_loss: 1.5330886840820312\n",
      "epoch: 150  train_loss: 1.2560065984725952\n",
      "epoch: 151  train_loss: 2.1777732372283936\n",
      "epoch: 152  train_loss: 1.353845238685608\n",
      "epoch: 153  train_loss: 1.2863669395446777\n",
      "epoch: 154  train_loss: 1.8967546224594116\n",
      "epoch: 155  train_loss: 1.582297921180725\n",
      "epoch: 156  train_loss: 1.4926213026046753\n",
      "epoch: 157  train_loss: 1.4298595190048218\n",
      "epoch: 158  train_loss: 1.61101496219635\n",
      "epoch: 159  train_loss: 1.7244504690170288\n",
      "epoch: 160  train_loss: 1.5116605758666992\n",
      "epoch: 161  train_loss: 1.438918113708496\n",
      "epoch: 162  train_loss: 1.3791651725769043\n",
      "epoch: 163  train_loss: 2.0697410106658936\n",
      "epoch: 164  train_loss: 1.2993173599243164\n",
      "epoch: 165  train_loss: 1.2592816352844238\n",
      "epoch: 166  train_loss: 2.128772735595703\n",
      "epoch: 167  train_loss: 1.1623202562332153\n",
      "epoch: 168  train_loss: 0.524843692779541\n",
      "epoch: 169  train_loss: 0.7663142085075378\n",
      "epoch: 170  train_loss: 0.7009892463684082\n",
      "epoch: 171  train_loss: 0.8580522537231445\n",
      "epoch: 172  train_loss: 0.625458300113678\n",
      "epoch: 173  train_loss: 0.8254334926605225\n",
      "epoch: 174  train_loss: 0.6995102763175964\n",
      "epoch: 175  train_loss: 1.1154512166976929\n",
      "epoch: 176  train_loss: 1.6029863357543945\n",
      "epoch: 177  train_loss: 2.1064846515655518\n",
      "epoch: 178  train_loss: 1.1728018522262573\n",
      "epoch: 179  train_loss: 0.7789462208747864\n",
      "epoch: 180  train_loss: 0.734237015247345\n",
      "epoch: 181  train_loss: 1.0784670114517212\n",
      "epoch: 182  train_loss: 1.5140680074691772\n",
      "epoch: 183  train_loss: 2.2292444705963135\n",
      "epoch: 184  train_loss: 1.0919809341430664\n",
      "epoch: 185  train_loss: 0.8417582511901855\n",
      "epoch: 186  train_loss: 0.5630525350570679\n",
      "epoch: 187  train_loss: 0.9195036292076111\n",
      "epoch: 188  train_loss: 1.3548803329467773\n",
      "epoch: 189  train_loss: 1.3581565618515015\n",
      "epoch: 190  train_loss: 2.379302740097046\n",
      "epoch: 191  train_loss: 1.3754191398620605\n",
      "epoch: 192  train_loss: 1.3614133596420288\n",
      "epoch: 193  train_loss: 2.470383882522583\n",
      "epoch: 194  train_loss: 2.062976598739624\n",
      "epoch: 195  train_loss: 2.7937443256378174\n",
      "epoch: 196  train_loss: 1.5635908842086792\n",
      "epoch: 197  train_loss: 1.052393913269043\n",
      "epoch: 198  train_loss: 0.938401460647583\n",
      "epoch: 199  train_loss: 1.2372759580612183\n",
      "epoch: 200  train_loss: 0.8427465558052063\n",
      "epoch: 201  train_loss: 0.5245563983917236\n",
      "epoch: 202  train_loss: 0.4955121576786041\n",
      "epoch: 203  train_loss: 0.4935263395309448\n",
      "epoch: 204  train_loss: 0.6691875457763672\n",
      "epoch: 205  train_loss: 0.7244055867195129\n",
      "epoch: 206  train_loss: 0.8993355631828308\n",
      "epoch: 207  train_loss: 1.4324373006820679\n",
      "epoch: 208  train_loss: 1.397469162940979\n",
      "epoch: 209  train_loss: 1.8061819076538086\n",
      "epoch: 210  train_loss: 1.3181235790252686\n",
      "epoch: 211  train_loss: 1.5814253091812134\n",
      "epoch: 212  train_loss: 1.4969749450683594\n",
      "epoch: 213  train_loss: 1.0404624938964844\n",
      "epoch: 214  train_loss: 2.246595621109009\n",
      "epoch: 215  train_loss: 1.2253361940383911\n",
      "epoch: 216  train_loss: 1.1655616760253906\n",
      "epoch: 217  train_loss: 2.0179357528686523\n",
      "epoch: 218  train_loss: 1.2856924533843994\n",
      "epoch: 219  train_loss: 1.4990237951278687\n",
      "epoch: 220  train_loss: 1.450107216835022\n",
      "epoch: 221  train_loss: 1.3110235929489136\n",
      "epoch: 222  train_loss: 1.8651617765426636\n",
      "epoch: 223  train_loss: 1.3658827543258667\n",
      "epoch: 224  train_loss: 1.3039005994796753\n",
      "epoch: 225  train_loss: 1.5367125272750854\n",
      "epoch: 226  train_loss: 1.7452977895736694\n",
      "epoch: 227  train_loss: 1.3160845041275024\n",
      "epoch: 228  train_loss: 1.2663370370864868\n",
      "epoch: 229  train_loss: 1.7840434312820435\n",
      "epoch: 230  train_loss: 1.3590952157974243\n",
      "epoch: 231  train_loss: 1.5033773183822632\n",
      "epoch: 232  train_loss: 1.4425328969955444\n",
      "epoch: 233  train_loss: 1.0422919988632202\n",
      "epoch: 234  train_loss: 2.2155754566192627\n",
      "epoch: 235  train_loss: 1.146891713142395\n",
      "epoch: 236  train_loss: 0.6637305617332458\n",
      "epoch: 237  train_loss: 0.7591875195503235\n",
      "epoch: 238  train_loss: 0.5653261542320251\n",
      "epoch: 239  train_loss: 0.8402621746063232\n",
      "epoch: 240  train_loss: 1.932054877281189\n",
      "epoch: 241  train_loss: 1.4424394369125366\n",
      "epoch: 242  train_loss: 1.3690372705459595\n",
      "epoch: 243  train_loss: 1.0054852962493896\n",
      "epoch: 244  train_loss: 0.8207586407661438\n",
      "epoch: 245  train_loss: 1.0031399726867676\n",
      "epoch: 246  train_loss: 1.3061431646347046\n",
      "epoch: 247  train_loss: 2.340301513671875\n",
      "epoch: 248  train_loss: 1.4717737436294556\n",
      "epoch: 249  train_loss: 3.2857582569122314\n",
      "epoch: 250  train_loss: 1.3735636472702026\n",
      "epoch: 251  train_loss: 1.2097128629684448\n",
      "epoch: 252  train_loss: 1.0108566284179688\n",
      "epoch: 253  train_loss: 0.5464017391204834\n",
      "epoch: 254  train_loss: 0.7709981799125671\n",
      "epoch: 255  train_loss: 1.040723443031311\n",
      "epoch: 256  train_loss: 1.1754547357559204\n",
      "epoch: 257  train_loss: 2.436924934387207\n",
      "epoch: 258  train_loss: 1.5114654302597046\n",
      "epoch: 259  train_loss: 3.3570029735565186\n",
      "epoch: 260  train_loss: 1.2404640913009644\n",
      "epoch: 261  train_loss: 1.1151220798492432\n",
      "epoch: 262  train_loss: 1.219166874885559\n",
      "epoch: 263  train_loss: 2.2485015392303467\n",
      "epoch: 264  train_loss: 1.0492113828659058\n",
      "epoch: 265  train_loss: 0.7473143935203552\n",
      "epoch: 266  train_loss: 0.6582636833190918\n",
      "epoch: 267  train_loss: 1.0358601808547974\n",
      "epoch: 268  train_loss: 1.457422137260437\n",
      "epoch: 269  train_loss: 1.9712826013565063\n",
      "epoch: 270  train_loss: 1.1146429777145386\n",
      "epoch: 271  train_loss: 0.6542519927024841\n",
      "epoch: 272  train_loss: 0.7355291247367859\n",
      "epoch: 273  train_loss: 0.5546512007713318\n",
      "epoch: 274  train_loss: 0.8782122135162354\n",
      "epoch: 275  train_loss: 1.8210997581481934\n",
      "epoch: 276  train_loss: 1.4241749048233032\n",
      "epoch: 277  train_loss: 1.3356413841247559\n",
      "epoch: 278  train_loss: 1.089577078819275\n",
      "epoch: 279  train_loss: 2.1002676486968994\n",
      "epoch: 280  train_loss: 1.1487308740615845\n",
      "epoch: 281  train_loss: 1.1207913160324097\n",
      "epoch: 282  train_loss: 2.001138687133789\n",
      "epoch: 283  train_loss: 1.128475308418274\n",
      "epoch: 284  train_loss: 1.4788860082626343\n",
      "epoch: 285  train_loss: 1.414380431175232\n",
      "epoch: 286  train_loss: 1.1127064228057861\n",
      "epoch: 287  train_loss: 1.9353910684585571\n",
      "epoch: 288  train_loss: 1.2358686923980713\n",
      "epoch: 289  train_loss: 1.1743265390396118\n",
      "epoch: 290  train_loss: 1.6331051588058472\n",
      "epoch: 291  train_loss: 1.506869912147522\n",
      "epoch: 292  train_loss: 1.3226429224014282\n",
      "epoch: 293  train_loss: 1.2672345638275146\n",
      "epoch: 294  train_loss: 1.5468244552612305\n",
      "epoch: 295  train_loss: 1.467179298400879\n",
      "epoch: 296  train_loss: 1.3897100687026978\n",
      "epoch: 297  train_loss: 1.330675482749939\n",
      "epoch: 298  train_loss: 1.1604915857315063\n",
      "epoch: 299  train_loss: 1.9540990591049194\n",
      "epoch: 300  train_loss: 1.1430190801620483\n",
      "epoch: 301  train_loss: 1.1069456338882446\n",
      "epoch: 302  train_loss: 1.975117802619934\n",
      "epoch: 303  train_loss: 1.0207089185714722\n",
      "epoch: 304  train_loss: 0.5398392081260681\n",
      "epoch: 305  train_loss: 0.7149252891540527\n",
      "epoch: 306  train_loss: 0.6215943694114685\n",
      "epoch: 307  train_loss: 0.9873736500740051\n",
      "epoch: 308  train_loss: 1.625801682472229\n",
      "epoch: 309  train_loss: 1.7195062637329102\n",
      "epoch: 310  train_loss: 1.140016794204712\n",
      "epoch: 311  train_loss: 0.8255136609077454\n",
      "epoch: 312  train_loss: 0.6378048062324524\n",
      "epoch: 313  train_loss: 0.687061607837677\n",
      "epoch: 314  train_loss: 0.5901467204093933\n",
      "epoch: 315  train_loss: 0.7214327454566956\n",
      "epoch: 316  train_loss: 0.7333968281745911\n",
      "epoch: 317  train_loss: 0.5904800295829773\n",
      "epoch: 318  train_loss: 0.5671548247337341\n",
      "epoch: 319  train_loss: 0.7524328827857971\n",
      "epoch: 320  train_loss: 0.7344045042991638\n",
      "epoch: 321  train_loss: 0.5836154818534851\n",
      "epoch: 322  train_loss: 0.5769925117492676\n",
      "epoch: 323  train_loss: 0.7410173416137695\n",
      "epoch: 324  train_loss: 0.7119832038879395\n",
      "epoch: 325  train_loss: 0.593834638595581\n",
      "epoch: 326  train_loss: 0.5963171124458313\n",
      "epoch: 327  train_loss: 0.7116015553474426\n",
      "epoch: 328  train_loss: 0.6659066081047058\n",
      "epoch: 329  train_loss: 0.6283890008926392\n",
      "epoch: 330  train_loss: 0.6283213496208191\n",
      "epoch: 331  train_loss: 0.6764483451843262\n",
      "epoch: 332  train_loss: 0.6526354551315308\n",
      "epoch: 333  train_loss: 0.6418095231056213\n",
      "epoch: 334  train_loss: 0.6454624533653259\n",
      "epoch: 335  train_loss: 0.6486436128616333\n",
      "epoch: 336  train_loss: 0.6120176911354065\n",
      "epoch: 337  train_loss: 0.6785151362419128\n",
      "epoch: 338  train_loss: 0.6832180619239807\n",
      "epoch: 339  train_loss: 0.6141095161437988\n",
      "epoch: 340  train_loss: 0.5841510891914368\n",
      "epoch: 341  train_loss: 0.7076306343078613\n",
      "epoch: 342  train_loss: 0.7190465927124023\n",
      "epoch: 343  train_loss: 0.594230592250824\n",
      "epoch: 344  train_loss: 0.5636928677558899\n",
      "epoch: 345  train_loss: 0.728642463684082\n",
      "epoch: 346  train_loss: 0.7335934042930603\n",
      "epoch: 347  train_loss: 0.5704163908958435\n",
      "epoch: 348  train_loss: 0.5497811436653137\n",
      "epoch: 349  train_loss: 0.7647588849067688\n",
      "epoch: 350  train_loss: 0.497152179479599\n",
      "epoch: 351  train_loss: 0.783822774887085\n",
      "epoch: 352  train_loss: 0.44984427094459534\n",
      "epoch: 353  train_loss: 0.4631824493408203\n",
      "epoch: 354  train_loss: 1.0720335245132446\n",
      "epoch: 355  train_loss: 1.1596814393997192\n",
      "epoch: 356  train_loss: 0.5480826497077942\n",
      "epoch: 357  train_loss: 0.4536825120449066\n",
      "epoch: 358  train_loss: 0.47084665298461914\n",
      "epoch: 359  train_loss: 0.5121968388557434\n",
      "epoch: 360  train_loss: 0.8689656257629395\n",
      "epoch: 361  train_loss: 1.2409754991531372\n",
      "epoch: 362  train_loss: 1.2354795932769775\n",
      "epoch: 363  train_loss: 1.9361900091171265\n",
      "epoch: 364  train_loss: 0.9185342788696289\n",
      "epoch: 365  train_loss: 0.5414368510246277\n",
      "epoch: 366  train_loss: 0.7741363644599915\n",
      "epoch: 367  train_loss: 0.6086933016777039\n",
      "epoch: 368  train_loss: 0.7067603468894958\n",
      "epoch: 369  train_loss: 1.6680856943130493\n",
      "epoch: 370  train_loss: 1.4926913976669312\n",
      "epoch: 371  train_loss: 1.4052343368530273\n",
      "epoch: 372  train_loss: 0.8143937587738037\n",
      "epoch: 373  train_loss: 0.5041904449462891\n",
      "epoch: 374  train_loss: 0.7448633313179016\n",
      "epoch: 375  train_loss: 0.5282784104347229\n",
      "epoch: 376  train_loss: 0.7092252373695374\n",
      "epoch: 377  train_loss: 0.6741597056388855\n",
      "epoch: 378  train_loss: 0.8473076224327087\n",
      "epoch: 379  train_loss: 1.1621723175048828\n",
      "epoch: 380  train_loss: 1.1846401691436768\n",
      "epoch: 381  train_loss: 2.262535810470581\n",
      "epoch: 382  train_loss: 1.915753722190857\n",
      "epoch: 383  train_loss: 2.7776005268096924\n",
      "epoch: 384  train_loss: 0.9947695136070251\n",
      "epoch: 385  train_loss: 1.1667715311050415\n",
      "epoch: 386  train_loss: 1.1623538732528687\n",
      "epoch: 387  train_loss: 2.089620590209961\n",
      "epoch: 388  train_loss: 0.725911557674408\n",
      "epoch: 389  train_loss: 0.5949137806892395\n",
      "epoch: 390  train_loss: 0.8422770500183105\n",
      "epoch: 391  train_loss: 1.2229723930358887\n",
      "epoch: 392  train_loss: 1.2186697721481323\n",
      "epoch: 393  train_loss: 1.8668009042739868\n",
      "epoch: 394  train_loss: 0.9350407123565674\n",
      "epoch: 395  train_loss: 0.5244877338409424\n",
      "epoch: 396  train_loss: 0.7429173588752747\n",
      "epoch: 397  train_loss: 0.49495139718055725\n",
      "epoch: 398  train_loss: 0.7053303718566895\n",
      "epoch: 399  train_loss: 0.5901598930358887\n",
      "epoch: 400  train_loss: 0.9627726674079895\n",
      "epoch: 401  train_loss: 1.5066308975219727\n",
      "epoch: 402  train_loss: 1.708639144897461\n",
      "epoch: 403  train_loss: 1.118863582611084\n",
      "epoch: 404  train_loss: 1.0824228525161743\n",
      "epoch: 405  train_loss: 1.925940990447998\n",
      "epoch: 406  train_loss: 1.0014625787734985\n",
      "epoch: 407  train_loss: 0.44689154624938965\n",
      "epoch: 408  train_loss: 0.8881235122680664\n",
      "epoch: 409  train_loss: 1.0972343683242798\n",
      "epoch: 410  train_loss: 0.673168957233429\n",
      "epoch: 411  train_loss: 0.6680379509925842\n",
      "epoch: 412  train_loss: 0.9847051501274109\n",
      "epoch: 413  train_loss: 1.057296872138977\n",
      "epoch: 414  train_loss: 2.3207879066467285\n",
      "epoch: 415  train_loss: 1.4128389358520508\n",
      "epoch: 416  train_loss: 3.103175163269043\n",
      "epoch: 417  train_loss: 1.1845214366912842\n",
      "epoch: 418  train_loss: 1.056064248085022\n",
      "epoch: 419  train_loss: 1.06106436252594\n",
      "epoch: 420  train_loss: 2.1882081031799316\n",
      "epoch: 421  train_loss: 0.9330996870994568\n",
      "epoch: 422  train_loss: 0.7288556694984436\n",
      "epoch: 423  train_loss: 0.482724666595459\n",
      "epoch: 424  train_loss: 0.7398448586463928\n",
      "epoch: 425  train_loss: 0.4697859287261963\n",
      "epoch: 426  train_loss: 0.7374103665351868\n",
      "epoch: 427  train_loss: 0.47851431369781494\n",
      "epoch: 428  train_loss: 0.7384545207023621\n",
      "epoch: 429  train_loss: 0.47107580304145813\n",
      "epoch: 430  train_loss: 0.7282273173332214\n",
      "epoch: 431  train_loss: 0.4789386987686157\n",
      "epoch: 432  train_loss: 0.7191821932792664\n",
      "epoch: 433  train_loss: 0.49136027693748474\n",
      "epoch: 434  train_loss: 0.7123550772666931\n",
      "epoch: 435  train_loss: 0.3249439299106598\n",
      "epoch: 436  train_loss: 0.7211397290229797\n",
      "epoch: 437  train_loss: 0.5551157593727112\n",
      "epoch: 438  train_loss: 0.6711795926094055\n",
      "epoch: 439  train_loss: 0.7481659054756165\n",
      "epoch: 440  train_loss: 0.4266127049922943\n",
      "epoch: 441  train_loss: 0.47875773906707764\n",
      "epoch: 442  train_loss: 0.9484243392944336\n",
      "epoch: 443  train_loss: 1.2185369729995728\n",
      "epoch: 444  train_loss: 0.47511807084083557\n",
      "epoch: 445  train_loss: 0.4603014290332794\n",
      "epoch: 446  train_loss: 0.9641322493553162\n",
      "epoch: 447  train_loss: 1.2347370386123657\n",
      "epoch: 448  train_loss: 0.4661159813404083\n",
      "epoch: 449  train_loss: 0.48742637038230896\n",
      "epoch: 450  train_loss: 0.8991072773933411\n",
      "epoch: 451  train_loss: 1.3043303489685059\n",
      "epoch: 452  train_loss: 0.437893271446228\n",
      "epoch: 453  train_loss: 0.879877507686615\n",
      "epoch: 454  train_loss: 1.075635552406311\n",
      "epoch: 455  train_loss: 0.6612213253974915\n",
      "epoch: 456  train_loss: 0.6461256146430969\n",
      "epoch: 457  train_loss: 0.9731006622314453\n",
      "epoch: 458  train_loss: 1.0357834100723267\n",
      "epoch: 459  train_loss: 2.2934515476226807\n",
      "epoch: 460  train_loss: 1.3904396295547485\n",
      "epoch: 461  train_loss: 3.0751733779907227\n",
      "epoch: 462  train_loss: 1.1595163345336914\n",
      "epoch: 463  train_loss: 1.0318635702133179\n",
      "epoch: 464  train_loss: 1.0572333335876465\n",
      "epoch: 465  train_loss: 2.1433019638061523\n",
      "epoch: 466  train_loss: 0.9200885891914368\n",
      "epoch: 467  train_loss: 0.7129265666007996\n",
      "epoch: 468  train_loss: 0.4725104868412018\n",
      "epoch: 469  train_loss: 0.7357311248779297\n",
      "epoch: 470  train_loss: 0.4535856246948242\n",
      "epoch: 471  train_loss: 0.735828697681427\n",
      "epoch: 472  train_loss: 0.4895443022251129\n",
      "epoch: 473  train_loss: 0.5772656798362732\n",
      "epoch: 474  train_loss: 0.6574830412864685\n",
      "epoch: 475  train_loss: 0.5891624093055725\n",
      "epoch: 476  train_loss: 0.6424430012702942\n",
      "epoch: 477  train_loss: 0.6513501405715942\n",
      "epoch: 478  train_loss: 0.573295533657074\n",
      "epoch: 479  train_loss: 0.5623254776000977\n",
      "epoch: 480  train_loss: 0.6561985015869141\n",
      "epoch: 481  train_loss: 0.6382219195365906\n",
      "epoch: 482  train_loss: 0.5792095065116882\n",
      "epoch: 483  train_loss: 0.5785054564476013\n",
      "epoch: 484  train_loss: 0.6417484283447266\n",
      "epoch: 485  train_loss: 0.618521511554718\n",
      "epoch: 486  train_loss: 0.5955071449279785\n",
      "epoch: 487  train_loss: 0.6036128997802734\n",
      "epoch: 488  train_loss: 0.6227299571037292\n",
      "epoch: 489  train_loss: 0.597923219203949\n",
      "epoch: 490  train_loss: 0.6035955548286438\n",
      "epoch: 491  train_loss: 0.6208906769752502\n",
      "epoch: 492  train_loss: 0.5973567366600037\n",
      "epoch: 493  train_loss: 0.5744564533233643\n",
      "epoch: 494  train_loss: 0.6315021514892578\n",
      "epoch: 495  train_loss: 0.6394898891448975\n",
      "epoch: 496  train_loss: 0.5685815811157227\n",
      "epoch: 497  train_loss: 0.5523290038108826\n",
      "epoch: 498  train_loss: 0.6681532859802246\n",
      "epoch: 499  train_loss: 0.663645327091217\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\n",
    "\"\"\"\n",
    "# # #  Dataset\n",
    "\"\"\"\n",
    "data_seed = 408\n",
    "np.random.seed(data_seed)\n",
    "batsize = 100\n",
    "n_dim = 20\n",
    "n_train = 300\n",
    "n_valid = 100\n",
    "n_test = 100\n",
    "\n",
    "budget = 1\n",
    "\n",
    "#nsim = 100  # number of datapoints: increase sample density for more robust results\n",
    "\n",
    "# create dictionaries with sampled datapoints with uniform distribution\n",
    "#data_loaded = np.load('portfolio_data/portfolio_var50_ineq50_eq1_ex12000.npz', allow_pickle=True)\n",
    "data_loaded = gen_portfolio_lto_data(n_dim,n_train,n_valid,n_test)\n",
    "Q_load = data_loaded['Q']\n",
    "A_load = data_loaded['A']\n",
    "G_load = data_loaded['G']\n",
    "h_load = data_loaded['h']\n",
    "x_load = data_loaded['x']\n",
    "p_train = data_loaded['trainX']\n",
    "p_valid = data_loaded['validX']\n",
    "p_test  = data_loaded['testX']\n",
    "sols_train = data_loaded['trainY']\n",
    "sols_valid = data_loaded['validY']\n",
    "sols_test  = data_loaded['testY']\n",
    "#feat_size_load = data_loaded['feat_size']\n",
    "\n",
    "print(\"p_train.dtype\")\n",
    "print( p_train.dtype )\n",
    "\n",
    "samples_train = {\"p\": torch.Tensor(p_train)}  # JK TODO fix this, reduced size for debugging\n",
    "samples_dev   = {\"p\": torch.Tensor(p_valid)}\n",
    "samples_test  = {\"p\": torch.Tensor(p_test )}\n",
    "\n",
    "# create named dictionary datasets\n",
    "train_data = DictDataset(samples_train, name='train')\n",
    "dev_data   = DictDataset(samples_dev,   name='dev')\n",
    "test_data  = DictDataset(samples_test,  name='test')\n",
    "# create torch dataloaders for the Trainer\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batsize, num_workers=0,\n",
    "                                            collate_fn=train_data.collate_fn, shuffle=True)\n",
    "dev_loader   = torch.utils.data.DataLoader(dev_data, batch_size=batsize, num_workers=0,\n",
    "                                            collate_fn=dev_data.collate_fn, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data, batch_size=batsize, num_workers=0,\n",
    "                                            collate_fn=test_data.collate_fn, shuffle=True)\n",
    "# note: training quality will depend on the DataLoader parameters such as batch size and shuffle\n",
    "\n",
    "\n",
    "\"\"\n",
    "\n",
    "\"\"\"\n",
    "# # #  pQP primal solution map architecture\n",
    "\"\"\"\n",
    "# define neural architecture for the solution map\n",
    "func = blocks.MLP(insize=n_dim, outsize=n_dim,\n",
    "                bias=True,\n",
    "                linear_map=slim.maps['linear'],\n",
    "                nonlin=nn.ReLU,\n",
    "                hsizes=[n_dim*2] * 4)\n",
    "# define symbolic solution map with concatenated features (problem parameters)\n",
    "#xi = lambda p1, p2: torch.cat([p1, p2], dim=-1)\n",
    "#features = Node(xi, ['p1', 'p2'], ['xi'], name='features')\n",
    "sol_map = Node(func, ['p'], ['x'], name='map')\n",
    "# trainable components of the problem solution\n",
    "components = [sol_map]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# # # objective and constraints formulation in Neuromancer\n",
    "\"\"\"\n",
    "# variables\n",
    "x = variable(\"x\")\n",
    "\n",
    "# sampled parameters\n",
    "p = variable('p')\n",
    "Q = torch.Tensor(Q_load)\n",
    "\n",
    "# objective function\n",
    "lambd = 1.0\n",
    "f = torch.sum(-p*x, dim = 1) + torch.sum( x*torch.matmul(Q,x.T).T, dim=1 ) #-p@x + lambd * x@Q@x\n",
    "obj = f.minimize(weight=1.0, name='obj')\n",
    "objectives = [obj]\n",
    "\n",
    "# constraints\n",
    "e = torch.ones(n_dim)\n",
    "Q_con = 100.\n",
    "con_1 = Q_con*(torch.sum(x, dim=1) == 1) #Q_con*(e@x == 1)\n",
    "con_1.name = 'c1'\n",
    "con_2 = Q_con * (x >= 0)\n",
    "con_2.name = 'c2'\n",
    "\n",
    "constraints = [con_1, con_2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# # #  problem formulation in Neuromancer\n",
    "\"\"\"\n",
    "# create penalty method loss function\n",
    "loss = PenaltyLoss(objectives, constraints)\n",
    "# construct constrained optimization problem\n",
    "problem = Problem(components, loss)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# # #  problem solution in Neuromancer\n",
    "\"\"\"\n",
    "optimizer = torch.optim.AdamW(problem.parameters(), lr=1e-3)\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    problem,\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    epochs=500,\n",
    "    patience=100,\n",
    "    warmup=100,\n",
    "    train_metric=\"train_loss\",\n",
    "    dev_metric=\"dev_loss\",\n",
    "    test_metric=\"test_loss\",\n",
    "    eval_metric=\"dev_loss\",\n",
    ")\n",
    "\n",
    "# Train solution map\n",
    "best_model = trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "#######################################\n",
    "#######################################\n",
    "\n",
    "Add on a ADMM correction layer\n",
    "\n",
    "#######################################\n",
    "########################################\n",
    "'''\n",
    "\n",
    "\n",
    "import ADMM as am\n",
    "from importlib import reload\n",
    "\n",
    "reload(am)\n",
    "\n",
    "\n",
    "'''\n",
    "# DEFINE THE OBJECTIVE\n",
    "'''\n",
    "# x is assumed to include slack variables!\n",
    "def f_obj(x,p):\n",
    "    x = x[:n_dim]\n",
    "    return -p@x + lambd * x@(Q@x)            #lambd * torch.sum(x*torch.mv(Q,x))   #torch.sum( -p*x  ) +          #-p@x + lambd * x@(Q@x)\n",
    "\n",
    "\n",
    "'''\n",
    "# DEFINE THE CONSTRAINTS\n",
    "'''\n",
    "def F_ineq(x,p):\n",
    "    x = x[:n_dim]\n",
    "    return -x\n",
    "\n",
    "def F_eq(x,p):\n",
    "    x = x[:n_dim]\n",
    "    return (x.sum() - budget).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "num_steps = 20 # number of ADMM iterations to take\n",
    "x_dim = n_dim # dimension of primal variable\n",
    "n_ineq = n_dim #number of inequality constraints\n",
    "n_eq = 1\n",
    "parm_dim = n_dim #number of problem parameters\n",
    "\n",
    "\n",
    "\n",
    "#metric scaling parameters\n",
    "#lb_P = 1.0/5.0\n",
    "#ub_P = 5.0\n",
    "#scl_lb_P = 0.05\n",
    "#scl_ub_P = 0.5\n",
    "\n",
    "lb_P = 1.0/100.0\n",
    "ub_P = 1.0\n",
    "scl_lb_P = 0.01\n",
    "scl_ub_P = 10.0\n",
    "\n",
    "met = am.ParametricDiagonal(x_dim+n_ineq,parm_dim,ub_P,lb_P,scl_ub_P,scl_lb_P )\n",
    "\n",
    "\n",
    "\n",
    "solver = am.ADMMSolverFast(\n",
    "    f_obj = f_obj, \n",
    "    F_ineq = F_ineq,\n",
    "    F_eq = F_eq,\n",
    "    x_dim = x_dim, \n",
    "    n_ineq = n_ineq,\n",
    "    n_eq = n_eq, \n",
    "    JF_fixed=True,\n",
    "    parm_dim = parm_dim,\n",
    "    num_steps = num_steps,\n",
    "    Metric = met\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# REMAP THROUGH CORRECTION\n",
    "sol_map = Node(func, ['p'], ['x_predicted'], name='map')\n",
    "ADMM_correction = Node(solver,['x_predicted','p'],['x','r_gap','s_gap','x_hist'])\n",
    "components = [sol_map, ADMM_correction]\n",
    "\n",
    "\n",
    "\n",
    "r_gap = variable(\"r_gap\")\n",
    "s_gap = variable(\"s_gap\")\n",
    "f_cnv = (r_gap)**2 + (s_gap)**2\n",
    "#f_cnv = (r_gap)**2\n",
    "#f_cnv = (s_gap)**2\n",
    "cnv_obj = f_cnv.minimize(weight=1e8, name='cnv_obj')\n",
    "objectives = [cnv_obj]\n",
    "constraints = []\n",
    "\n",
    "# create loss function\n",
    "loss = PenaltyLoss(objectives, constraints)\n",
    "# construct constrained optimization problem\n",
    "problem = Problem(components, loss)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0  train_loss: 43.09910202026367\n",
      "epoch: 1  train_loss: 32.26182174682617\n",
      "epoch: 2  train_loss: 24.5458984375\n",
      "epoch: 3  train_loss: 18.531280517578125\n",
      "epoch: 4  train_loss: 14.860871315002441\n",
      "epoch: 5  train_loss: 12.171974182128906\n",
      "epoch: 6  train_loss: 10.435139656066895\n",
      "epoch: 7  train_loss: 9.152556419372559\n",
      "epoch: 8  train_loss: 8.124173164367676\n",
      "epoch: 9  train_loss: 7.255682468414307\n",
      "epoch: 10  train_loss: 6.599167346954346\n",
      "epoch: 11  train_loss: 5.988189697265625\n",
      "epoch: 12  train_loss: 5.4990081787109375\n",
      "epoch: 13  train_loss: 5.114609241485596\n",
      "epoch: 14  train_loss: 4.8323893547058105\n",
      "epoch: 15  train_loss: 4.591391563415527\n",
      "epoch: 16  train_loss: 4.434464931488037\n",
      "epoch: 17  train_loss: 4.294129848480225\n",
      "epoch: 18  train_loss: 4.171027660369873\n",
      "epoch: 19  train_loss: 4.103302955627441\n",
      "epoch: 20  train_loss: 4.018889904022217\n",
      "epoch: 21  train_loss: 3.9618875980377197\n",
      "epoch: 22  train_loss: 3.9248945713043213\n",
      "epoch: 23  train_loss: 3.890366315841675\n",
      "epoch: 24  train_loss: 3.8560237884521484\n",
      "epoch: 25  train_loss: 3.7973434925079346\n",
      "epoch: 26  train_loss: 3.7910335063934326\n",
      "epoch: 27  train_loss: 3.746976613998413\n",
      "epoch: 28  train_loss: 3.7390592098236084\n",
      "epoch: 29  train_loss: 3.7075083255767822\n",
      "epoch: 30  train_loss: 3.674860715866089\n",
      "epoch: 31  train_loss: 3.6606500148773193\n",
      "epoch: 32  train_loss: 3.6302969455718994\n",
      "epoch: 33  train_loss: 3.612074613571167\n",
      "epoch: 34  train_loss: 3.583470582962036\n",
      "epoch: 35  train_loss: 3.5445430278778076\n",
      "epoch: 36  train_loss: 3.525912046432495\n",
      "epoch: 37  train_loss: 3.4804131984710693\n",
      "Interrupted training loop.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#######################################\n",
    "#######################################\n",
    "\n",
    "Train ADMM correction layer\n",
    "\n",
    "#######################################\n",
    "########################################\n",
    "'''\n",
    "optimizer = torch.optim.AdamW(solver.parameters(), lr=1e-3)\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    problem,\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    epochs=100,\n",
    "    patience=200,\n",
    "    warmup=100,\n",
    "    train_metric=\"train_loss\",\n",
    "    dev_metric=\"dev_loss\",\n",
    "    test_metric=\"test_loss\",\n",
    "    eval_metric=\"dev_loss\",\n",
    ")\n",
    "# Train solution map\n",
    "best_model = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.num_steps =20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Solution Difference: 6.7145024e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = time.time()\n",
    "with torch.no_grad():\n",
    "    samples_test['name'] = 'test'\n",
    "    model_out = problem(samples_test)\n",
    "nm_time = time.time() - t\n",
    "\n",
    "\n",
    "x_nm_test = model_out['test_' + \"x\"].detach().numpy()\n",
    "x_loaded_test  = data_loaded['testY']\n",
    "\n",
    "#print(\"samples_test['p']\")\n",
    "#print(samples_test['p'])\n",
    "\n",
    "cvxpy_layer = cvx_qp(n_dim,Q,budget)\n",
    "t = time.time()\n",
    "x_cvxpy_test = cvxpy_layer(samples_test['p'])\n",
    "cv_time = time.time() -t\n",
    "\n",
    "#print(\"x_nm_test\")\n",
    "#print( x_nm_test )\n",
    "#print(\"x_loaded_test\")\n",
    "#print( x_loaded_test )\n",
    "#print(\"x_cvxpy_test\")\n",
    "#print( x_cvxpy_test )\n",
    "\n",
    "\n",
    "#print('cv/nm time:',cv_time/nm_time)\n",
    "\n",
    "sol_diff = np.mean(np.sum((x_cvxpy_test.detach().numpy() - x_nm_test)**2,axis=-1))\n",
    "print(\"Average Solution Difference:\",np.mean(sol_diff))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# '''\n",
    "# Compute Train set error\n",
    "# '''\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     t = time.time()\n",
    "#     samples_train['name'] = 'train'\n",
    "#     model_out = problem(samples_train)\n",
    "#     nm_time = time.time() - t\n",
    "\n",
    "\n",
    "# x_nm_test = model_out['train_' + \"x\"].detach().numpy()\n",
    "# x_loaded_test  = data_loaded['trainY']\n",
    "\n",
    "\n",
    "# cvxpy_layer = cvx_qp(n_dim,Q)\n",
    "# x_cvxpy_test = cvxpy_layer(samples_train['p'])\n",
    "\n",
    "# # print(\"x_nm_test\")\n",
    "# # print( x_nm_test )\n",
    "# # print(\"x_loaded_test\")\n",
    "# # print( x_loaded_test )\n",
    "# # print(\"x_cvxpy_test\")\n",
    "# # print( x_cvxpy_test )\n",
    "\n",
    "# sol_diff = np.mean(np.sum((x_cvxpy_test.detach().numpy() - x_nm_test)**2,axis=-1))\n",
    "# print(\"Average Solution Difference:\",np.mean(sol_diff))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0786, -0.1396,  0.0544,  ..., -0.0198,  0.0820, -0.1486],\n",
       "        [ 0.1672, -0.0667,  0.1019,  ..., -0.0181,  0.4626, -0.1047],\n",
       "        [-0.3325, -0.2108, -0.3959,  ...,  0.1514, -0.1867, -0.1235],\n",
       "        ...,\n",
       "        [ 0.1383, -0.1055,  0.1441,  ...,  0.0018,  0.0623,  0.3851],\n",
       "        [-0.1949, -0.4182,  0.0315,  ..., -0.4287,  0.2174,  0.0565],\n",
       "        [-0.2034, -0.2070,  0.5283,  ..., -0.1636, -0.1470,  0.3831]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_test['p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20)\n",
      "(21, 100, 20)\n",
      "(21, 100, 20)\n",
      "(21,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'number of iterations')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Convergence Evaluation\n",
    "'''\n",
    "model_out = problem(samples_test)\n",
    "x_cvxpy = x_cvxpy_test.detach().numpy()\n",
    "print(x_cvxpy.shape) #(batch, x_dim)\n",
    "x_cvxpy = np.expand_dims(x_cvxpy,0)\n",
    "x_cvxpy = np.tile(x_cvxpy,(solver.num_steps + 1,1,1))\n",
    "print(x_cvxpy.shape)\n",
    "\n",
    "x_hist = model_out['test_x_hist']\n",
    "x_hist = torch.stack(x_hist).detach().numpy()\n",
    "x_hist = x_hist[:,:,0:n_dim]\n",
    "print(x_hist.shape) #(num of DR steps, batch size, x_dim)\n",
    "\n",
    "\n",
    "\n",
    "cnvg_gap = np.mean(np.sum( (x_cvxpy - x_hist)**2, axis = 2),axis = 1)\n",
    "print(cnvg_gap.shape)\n",
    "\n",
    "plt.plot(np.log10(cnvg_gap))\n",
    "plt.ylabel('mean log MSE difference from true solution')\n",
    "plt.xlabel('number of iterations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [1, 100] but got: [1, 20].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 72\u001b[0m\n\u001b[1;32m     69\u001b[0m test_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(test_x,\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     70\u001b[0m test_p \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munsqueeze(test_p,\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 72\u001b[0m x_hist \u001b[38;5;241m=\u001b[39m \u001b[43msolver\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_p\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     73\u001b[0m x_hist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(x_hist)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     74\u001b[0m x_hist \u001b[38;5;241m=\u001b[39m x_hist[:,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m100\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/DAIDIST/LOPO/New_Experiments/ADMM.py:330\u001b[0m, in \u001b[0;36mADMMSolverFast.forward\u001b[0;34m(self, x, parms)\u001b[0m\n\u001b[1;32m    328\u001b[0m x_hist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    329\u001b[0m x_hist\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m--> 330\u001b[0m Lcho, Md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfoF_Id\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_steps):\n\u001b[1;32m    332\u001b[0m     y_k_old \u001b[38;5;241m=\u001b[39m y_k\n",
      "File \u001b[0;32m~/Projects/DAIDIST/LOPO/New_Experiments/Prox.py:331\u001b[0m, in \u001b[0;36mSecondOrderObjectiveConstraintCompositionFixedH.prefactor\u001b[0;34m(self, x, parms)\u001b[0m\n\u001b[1;32m    329\u001b[0m Qn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQn,(batch_size,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# Compute the Hessian of f\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m Hf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mH_vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m### Compute the gradient step and Projection Operation\u001b[39;00m\n\u001b[1;32m    333\u001b[0m Pm \u001b[38;5;241m=\u001b[39m vmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric)(x,parms)\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/_functorch/apis.py:188\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/_functorch/vmap.py:266\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    263\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/_functorch/vmap.py:38\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/_functorch/vmap.py:379\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 379\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:1131\u001b[0m, in \u001b[0;36mjacfwd.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m   1128\u001b[0m     _, jvp_out \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jvp_out\n\u001b[0;32m-> 1131\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpush_jvp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandomness\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1133\u001b[0m     results, aux \u001b[38;5;241m=\u001b[39m results\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/_functorch/apis.py:188\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/_functorch/vmap.py:266\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(func, flat_in_dims, chunks_flat_args,\n\u001b[1;32m    263\u001b[0m                          args_spec, out_dims, randomness, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/_functorch/vmap.py:38\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/_functorch/vmap.py:379\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(flat_in_dims, flat_args, vmap_level, args_spec)\n\u001b[0;32m--> 379\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:1122\u001b[0m, in \u001b[0;36mjacfwd.<locals>.wrapper_fn.<locals>.push_jvp\u001b[0;34m(basis)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpush_jvp\u001b[39m(basis):\n\u001b[0;32m-> 1122\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43m_jvp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;66;03m# output[0] is the output of `func(*args)`\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m     error_if_complex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacfwd\u001b[39m\u001b[38;5;124m\"\u001b[39m, output[\u001b[38;5;241m0\u001b[39m], is_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/_functorch/vmap.py:38\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:968\u001b[0m, in \u001b[0;36m_jvp_with_argnums\u001b[0;34m(func, primals, tangents, argnums, strict, has_aux)\u001b[0m\n\u001b[1;32m    966\u001b[0m     primals \u001b[38;5;241m=\u001b[39m _wrap_all_tensors(primals, level)\n\u001b[1;32m    967\u001b[0m     duals \u001b[38;5;241m=\u001b[39m _replace_args(primals, duals, argnums)\n\u001b[0;32m--> 968\u001b[0m result_duals \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mduals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(result_duals, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result_duals) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:492\u001b[0m, in \u001b[0;36mjacrev.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    491\u001b[0m     error_if_complex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacrev\u001b[39m\u001b[38;5;124m\"\u001b[39m, args, is_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 492\u001b[0m     vjp_out \u001b[38;5;241m=\u001b[39m \u001b[43m_vjp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    494\u001b[0m         output, vjp_fn, aux \u001b[38;5;241m=\u001b[39m vjp_out\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/_functorch/vmap.py:38\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/DOPX_layers/lib/python3.9/site-packages/torch/_functorch/eager_transforms.py:294\u001b[0m, in \u001b[0;36m_vjp_with_argnums\u001b[0;34m(func, argnums, has_aux, *primals)\u001b[0m\n\u001b[1;32m    292\u001b[0m     diff_primals \u001b[38;5;241m=\u001b[39m _slice_argnums(primals, argnums, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    293\u001b[0m     tree_map_(partial(_create_differentiable, level\u001b[38;5;241m=\u001b[39mlevel), diff_primals)\n\u001b[0;32m--> 294\u001b[0m primals_out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(primals_out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(primals_out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m~/Projects/DAIDIST/LOPO/New_Experiments/ADMM.py:299\u001b[0m, in \u001b[0;36mADMMSolverFast.__init__.<locals>.f\u001b[0;34m(xs, parms)\u001b[0m\n\u001b[1;32m    297\u001b[0m x \u001b[38;5;241m=\u001b[39m xs[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_dim]\n\u001b[1;32m    298\u001b[0m s \u001b[38;5;241m=\u001b[39m xs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_dim:]\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparms\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mf_obj\u001b[0;34m(x, p)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf_obj\u001b[39m(x,p):\n\u001b[1;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m x[:n_dim]\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241;43m-\u001b[39;49m\u001b[43mp\u001b[49m\u001b[38;5;129;43m@x\u001b[39;49m \u001b[38;5;241m+\u001b[39m lambd \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m@\u001b[39m(Q\u001b[38;5;129m@x\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [1, 100] but got: [1, 20]."
     ]
    }
   ],
   "source": [
    "'''\n",
    "Convergence Evaluation\n",
    "'''\n",
    "solver.num_steps = 30\n",
    "\n",
    "# test_p = torch.tensor([ 0.0786, -0.1396,  0.0544,  0.5873, -0.1937,  0.0593,  0.2089, -0.2672,\n",
    "#          0.2413,  0.1833, -0.1695,  0.1558,  0.3340,  0.2615, -0.0777, -0.2420,\n",
    "#         -0.4748, -0.0198,  0.0820, -0.1486],dtype = torch.float32)\n",
    "\n",
    "# test_x = torch.tensor([0.0571, 0.0828, 0.1068, 0.0153, 0.0139, 0.0655, 0.0151, 0.0279, 0.0574,\n",
    "#         0.0053, 0.0264, 0.0175, 0.0355, 0.0876, 0.0247, 0.0498, 0.0314, 0.0840,\n",
    "#         0.0746, 0.1085],dtype = torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_p =torch.tensor([ 1.7857e-01, -2.7497e-01,  2.5717e-01,  3.8364e-02, -7.7331e-04,\n",
    "        -2.9028e-01,  3.3549e-02, -1.1741e-01, -2.1641e-02, -3.5141e-02,\n",
    "         7.9165e-02, -5.1123e-01,  4.4745e-01,  3.6999e-01,  3.5046e-02,\n",
    "         2.5234e-01,  2.3587e-01,  7.4071e-02, -2.3285e-01,  1.9717e-01,\n",
    "        -9.9756e-02,  1.3691e-01,  1.6738e-01, -2.2785e-01,  4.0373e-02,\n",
    "         5.2640e-03,  4.6145e-01, -8.4815e-01,  1.6224e-03, -4.8091e-01,\n",
    "        -1.0678e-01,  1.6898e-01,  3.6447e-01,  4.2021e-01,  1.3222e-01,\n",
    "        -1.3869e-01,  2.1727e-01, -2.9587e-01,  1.4124e-01,  3.8298e-01,\n",
    "         1.3537e-01,  4.8293e-01, -3.2068e-01, -2.2352e-01, -2.5423e-01,\n",
    "         8.5373e-02, -6.7962e-02, -9.7984e-02, -5.3502e-02,  2.7891e-01,\n",
    "         2.8244e-01, -1.3083e-01,  7.4079e-02, -4.5340e-01, -3.0208e-01,\n",
    "        -6.0530e-01,  3.0580e-01,  3.5115e-02, -5.4756e-01, -1.1652e-01,\n",
    "         2.7381e-01,  4.9779e-01,  3.9558e-01,  1.3829e-01, -5.0766e-01,\n",
    "         1.6634e-01, -2.1521e-02,  3.3210e-01,  1.8912e-02,  2.3219e-02,\n",
    "        -5.5921e-01, -1.7104e-01, -2.2898e-01, -2.1905e-01, -1.8726e-01,\n",
    "        -7.9069e-02, -3.5976e-02,  5.2348e-01, -4.7270e-02,  1.5996e-01,\n",
    "        -3.7088e-01, -1.5538e-01, -2.1970e-01, -8.4674e-02, -9.5221e-02,\n",
    "         3.5431e-01,  9.1328e-03,  1.5133e-01,  3.2873e-01, -1.3085e-02,\n",
    "         2.6877e-01, -6.6001e-02, -1.6084e-02,  2.0000e-01, -9.7725e-02,\n",
    "        -6.0260e-02,  7.6604e-02,  1.1087e-01, -1.0653e-01, -2.5936e-01],dtype = torch.float32)\n",
    "\n",
    "test_x = torch.tensor([ 2.8652e-02, -1.8720e-02,  4.5943e-03,  2.0666e-02,  5.3052e-02,\n",
    "         1.0566e-02, -2.3301e-04,  7.4269e-03, -4.0737e-03,  4.0351e-02,\n",
    "         3.3887e-03, -2.8895e-02, -8.0340e-03,  5.5137e-02,  2.0804e-02,\n",
    "         5.8592e-03, -9.7132e-05,  4.8359e-02, -3.4225e-02, -2.6139e-02,\n",
    "         2.7030e-02, -2.8122e-02, -1.9087e-02,  1.0089e-04,  2.0335e-02,\n",
    "         5.0663e-03, -1.5315e-02, -1.1443e-02,  4.8434e-02, -9.4290e-03,\n",
    "        -3.4455e-03,  1.2143e-02, -3.3611e-03,  2.4777e-04,  5.6196e-03,\n",
    "        -1.0082e-02,  6.5460e-02,  7.1221e-05, -2.5061e-02,  5.1259e-02,\n",
    "         1.0274e-04,  5.6792e-02,  5.8215e-02, -1.1410e-03,  3.6398e-02,\n",
    "         1.1078e-02,  4.5247e-02,  1.8908e-02,  2.3678e-03,  3.5135e-03,\n",
    "         4.1801e-02, -4.6820e-03,  6.3582e-04,  1.6950e-02, -1.3386e-02,\n",
    "         1.8284e-02, -2.0947e-02,  3.6561e-02,  6.6216e-02, -8.3138e-04,\n",
    "         7.6932e-05, -7.4624e-03, -1.5951e-02,  1.4933e-02, -3.3135e-02,\n",
    "         1.6421e-02,  4.4886e-02,  4.2690e-02,  4.6706e-03, -6.9701e-03,\n",
    "        -1.9117e-02, -4.8848e-04, -3.0347e-02,  1.7936e-02, -2.6791e-04,\n",
    "         5.1786e-02, -6.4215e-04,  3.8023e-03,  4.3435e-02, -7.2335e-03,\n",
    "         9.9769e-05,  1.1077e-02,  4.4773e-02,  1.0966e-02, -1.7365e-03,\n",
    "        -9.9912e-03,  8.1190e-03, -2.1046e-04,  4.0432e-02,  1.9598e-02,\n",
    "        -1.6820e-02, -1.0310e-02,  4.5409e-02,  5.8548e-03, -1.7351e-02,\n",
    "        -2.0528e-02, -1.2997e-02,  6.0960e-03,  3.6543e-02,  3.6034e-02],dtype = torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "#test_p = torch.tensor([p1,p2],dtype=torch.float32)\n",
    "#test_x = torch.tensor([0.32,-0.37],dtype=torch.float32)\n",
    "# cvxpy_layer = cvx_qp(n_dim,Q,budget)\n",
    "# x_cvxpy_test = cvxpy_layer(test_p)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_x = torch.unsqueeze(test_x,0)\n",
    "test_p = torch.unsqueeze(test_p,0)\n",
    "\n",
    "x_hist = solver(test_x,test_p)[3]\n",
    "x_hist = torch.stack(x_hist).detach().numpy()\n",
    "x_hist = x_hist[:,0,0:100]\n",
    "\n",
    "\n",
    "#np.save('Convergence_Data/portfolio100_ADMM.npy',x_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2dd1a94c0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.log10(np.sum((x_hist - x_cvxpy_test.detach().numpy()[0,:])**2,axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.66545618e-01, -2.75455075e-10, -1.22103128e-09,  1.45438433e-01,\n",
       "       -1.94059199e-10,  1.57137647e-01,  1.19169511e-01,  7.75483176e-02,\n",
       "        1.82705835e-01, -2.88751995e-10,  1.75828323e-01,  3.30099696e-03,\n",
       "        1.90075964e-01,  1.85005844e-01,  3.82080562e-02,  2.57342070e-01,\n",
       "        2.06850827e-01,  1.20815210e-01,  2.11700760e-02,  2.31784016e-01,\n",
       "        9.49936435e-02,  1.93888575e-01,  1.63921803e-01,  3.91377062e-02,\n",
       "        1.81399226e-01,  1.46353126e-01,  1.13306241e-02,  8.39460237e-11,\n",
       "        6.46391511e-02,  5.64657412e-02,  9.80337039e-02,  1.68516189e-01,\n",
       "        2.58844167e-01,  2.43572026e-01,  1.26059011e-01,  1.25921756e-01,\n",
       "        2.25579426e-01,  4.47541177e-02,  1.83007181e-01,  2.86692709e-01,\n",
       "        1.63639292e-01,  2.50862688e-01, -1.73734915e-10,  2.04896331e-02,\n",
       "        4.98930849e-02,  2.13674903e-01,  6.30491897e-02,  1.26736119e-01,\n",
       "        1.44963592e-01, -3.96767952e-09,  5.95956743e-02,  5.39553091e-02,\n",
       "        2.88749374e-02, -3.49279564e-11,  1.15862444e-01, -2.62037558e-10,\n",
       "        1.00085378e-01,  7.57316798e-02,  8.34273081e-03,  7.28240833e-02,\n",
       "        1.53037757e-01,  2.98038155e-01, -1.33423050e-09,  5.79207242e-02,\n",
       "       -5.11358629e-11, -5.38361411e-10, -5.38685985e-10,  1.03775471e-01,\n",
       "        1.29845396e-01, -1.74461445e-10, -2.60454408e-10,  9.51446686e-03,\n",
       "        3.01616509e-02, -6.95964175e-10,  1.58105373e-01,  1.78292036e-01,\n",
       "        1.31993055e-01,  1.10217132e-01,  6.33413419e-02,  9.62476060e-02,\n",
       "        6.82356730e-02, -6.39346576e-10,  1.10612683e-01,  1.29709333e-01,\n",
       "        1.64903596e-01,  2.79470474e-01,  3.60623784e-02,  1.87986717e-01,\n",
       "        4.84251454e-02, -4.31465641e-10, -6.49101495e-10,  1.12309463e-01,\n",
       "        8.28726441e-02,  1.57699347e-01, -9.80275305e-11,  4.79299463e-02,\n",
       "       -1.71235248e-10,  1.48030326e-01,  9.20662582e-02,  7.25785494e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cvxpy_test.detach().numpy()[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuromancer_functorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
