{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLearning to solve parametric Quadratic Programming\\nportfolio optimization\\nproblems using Neuromancer.\\n\\nProblem formulation:\\n    minimize    - p^T x + lambda x^T Q x\\n    subject to       1^T x = 1\\n                      x >= 0\\n\\nWhere p is interpreted as a vector of asset returns, and Q represents\\nthe covariance between assets, which forms a penalty on overall\\ncovariance (risk) weighted by lambda.\\n'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Learning to solve parametric Quadratic Programming\n",
    "portfolio optimization\n",
    "problems using Neuromancer.\n",
    "\n",
    "Problem formulation:\n",
    "    minimize    - p^T x + lambda x^T Q x\n",
    "    subject to       1^T x = 1\n",
    "                      x >= 0\n",
    "\n",
    "Where p is interpreted as a vector of asset returns, and Q represents\n",
    "the covariance between assets, which forms a penalty on overall\n",
    "covariance (risk) weighted by lambda.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import neuromancer.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as patheffects\n",
    "\n",
    "from neuromancer.trainer import Trainer\n",
    "from neuromancer.problem import Problem\n",
    "from neuromancer.constraint import variable\n",
    "from neuromancer.dataset import DictDataset\n",
    "from neuromancer.loss import PenaltyLoss\n",
    "from neuromancer.modules import blocks\n",
    "from neuromancer.system import Node\n",
    "\n",
    "from portfolio_utils import gen_portfolio_lto_data, cvx_qp\n",
    "import cvxpy as cp\n",
    "from cvxpylayers.torch.cvxpylayer import CvxpyLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset . . . \n",
      "Training set generated \n",
      "Validation set generated \n",
      "Test set generated \n",
      "p_train.dtype\n",
      "float64\n",
      "epoch: 0  train_loss: 1057.9630126953125\n",
      "epoch: 1  train_loss: 1034.7286376953125\n",
      "epoch: 2  train_loss: 1011.7421875\n",
      "epoch: 3  train_loss: 987.485595703125\n",
      "epoch: 4  train_loss: 961.0724487304688\n",
      "epoch: 5  train_loss: 931.0846557617188\n",
      "epoch: 6  train_loss: 896.1134643554688\n",
      "epoch: 7  train_loss: 854.687255859375\n",
      "epoch: 8  train_loss: 803.5391235351562\n",
      "epoch: 9  train_loss: 739.625732421875\n",
      "epoch: 10  train_loss: 658.5283203125\n",
      "epoch: 11  train_loss: 555.737060546875\n",
      "epoch: 12  train_loss: 434.2226257324219\n",
      "epoch: 13  train_loss: 294.0727844238281\n",
      "epoch: 14  train_loss: 162.1291961669922\n",
      "epoch: 15  train_loss: 212.39634704589844\n",
      "epoch: 16  train_loss: 244.1176300048828\n",
      "epoch: 17  train_loss: 184.36474609375\n",
      "epoch: 18  train_loss: 114.07989501953125\n",
      "epoch: 19  train_loss: 119.22830963134766\n",
      "epoch: 20  train_loss: 110.28784942626953\n",
      "epoch: 21  train_loss: 84.74121856689453\n",
      "epoch: 22  train_loss: 83.4977035522461\n",
      "epoch: 23  train_loss: 66.51051330566406\n",
      "epoch: 24  train_loss: 62.15205001831055\n",
      "epoch: 25  train_loss: 53.207244873046875\n",
      "epoch: 26  train_loss: 47.28578567504883\n",
      "epoch: 27  train_loss: 43.75102233886719\n",
      "epoch: 28  train_loss: 39.60392379760742\n",
      "epoch: 29  train_loss: 40.17036819458008\n",
      "epoch: 30  train_loss: 35.94105529785156\n",
      "epoch: 31  train_loss: 44.519439697265625\n",
      "epoch: 32  train_loss: 36.836219787597656\n",
      "epoch: 33  train_loss: 35.221370697021484\n",
      "epoch: 34  train_loss: 34.12859344482422\n",
      "epoch: 35  train_loss: 31.430871963500977\n",
      "epoch: 36  train_loss: 30.663862228393555\n",
      "epoch: 37  train_loss: 26.658864974975586\n",
      "epoch: 38  train_loss: 26.803346633911133\n",
      "epoch: 39  train_loss: 26.23686408996582\n",
      "epoch: 40  train_loss: 25.152780532836914\n",
      "epoch: 41  train_loss: 25.44007110595703\n",
      "epoch: 42  train_loss: 25.46894073486328\n",
      "epoch: 43  train_loss: 23.871566772460938\n",
      "epoch: 44  train_loss: 26.07488250732422\n",
      "epoch: 45  train_loss: 22.6595458984375\n",
      "epoch: 46  train_loss: 23.672372817993164\n",
      "epoch: 47  train_loss: 27.15880012512207\n",
      "epoch: 48  train_loss: 27.788423538208008\n",
      "epoch: 49  train_loss: 27.362829208374023\n",
      "epoch: 50  train_loss: 37.192222595214844\n",
      "epoch: 51  train_loss: 27.583044052124023\n",
      "epoch: 52  train_loss: 32.69065856933594\n",
      "epoch: 53  train_loss: 27.903898239135742\n",
      "epoch: 54  train_loss: 28.38169288635254\n",
      "epoch: 55  train_loss: 34.1760139465332\n",
      "epoch: 56  train_loss: 26.790651321411133\n",
      "epoch: 57  train_loss: 29.277196884155273\n",
      "epoch: 58  train_loss: 20.916292190551758\n",
      "epoch: 59  train_loss: 21.26869773864746\n",
      "epoch: 60  train_loss: 25.679550170898438\n",
      "epoch: 61  train_loss: 26.93382453918457\n",
      "epoch: 62  train_loss: 26.15846061706543\n",
      "epoch: 63  train_loss: 21.25400161743164\n",
      "epoch: 64  train_loss: 19.636503219604492\n",
      "epoch: 65  train_loss: 20.307340621948242\n",
      "epoch: 66  train_loss: 20.2106876373291\n",
      "epoch: 67  train_loss: 19.674972534179688\n",
      "epoch: 68  train_loss: 20.008869171142578\n",
      "epoch: 69  train_loss: 19.517919540405273\n",
      "epoch: 70  train_loss: 19.30845069885254\n",
      "epoch: 71  train_loss: 18.300369262695312\n",
      "epoch: 72  train_loss: 20.089351654052734\n",
      "epoch: 73  train_loss: 21.807676315307617\n",
      "epoch: 74  train_loss: 22.1965274810791\n",
      "epoch: 75  train_loss: 20.60524559020996\n",
      "epoch: 76  train_loss: 27.04145622253418\n",
      "epoch: 77  train_loss: 24.329641342163086\n",
      "epoch: 78  train_loss: 24.8642578125\n",
      "epoch: 79  train_loss: 23.87420082092285\n",
      "epoch: 80  train_loss: 28.979055404663086\n",
      "epoch: 81  train_loss: 23.689559936523438\n",
      "epoch: 82  train_loss: 23.294607162475586\n",
      "epoch: 83  train_loss: 25.8714599609375\n",
      "epoch: 84  train_loss: 24.73455238342285\n",
      "epoch: 85  train_loss: 23.244237899780273\n",
      "epoch: 86  train_loss: 22.410181045532227\n",
      "epoch: 87  train_loss: 25.389404296875\n",
      "epoch: 88  train_loss: 23.204954147338867\n",
      "epoch: 89  train_loss: 24.184751510620117\n",
      "epoch: 90  train_loss: 22.915456771850586\n",
      "epoch: 91  train_loss: 21.299551010131836\n",
      "epoch: 92  train_loss: 26.58479881286621\n",
      "epoch: 93  train_loss: 23.53226661682129\n",
      "epoch: 94  train_loss: 23.499788284301758\n",
      "epoch: 95  train_loss: 23.193153381347656\n",
      "epoch: 96  train_loss: 27.435258865356445\n",
      "epoch: 97  train_loss: 22.604576110839844\n",
      "epoch: 98  train_loss: 21.94474983215332\n",
      "epoch: 99  train_loss: 24.950227737426758\n",
      "epoch: 100  train_loss: 24.713462829589844\n",
      "epoch: 101  train_loss: 22.97614860534668\n",
      "epoch: 102  train_loss: 21.980804443359375\n",
      "epoch: 103  train_loss: 25.425033569335938\n",
      "epoch: 104  train_loss: 22.7463436126709\n",
      "epoch: 105  train_loss: 23.508407592773438\n",
      "epoch: 106  train_loss: 22.95184326171875\n",
      "epoch: 107  train_loss: 20.969636917114258\n",
      "epoch: 108  train_loss: 27.81622314453125\n",
      "epoch: 109  train_loss: 21.22418785095215\n",
      "epoch: 110  train_loss: 21.2396297454834\n",
      "epoch: 111  train_loss: 26.39689826965332\n",
      "epoch: 112  train_loss: 22.531982421875\n",
      "epoch: 113  train_loss: 23.920351028442383\n",
      "epoch: 114  train_loss: 22.829612731933594\n",
      "epoch: 115  train_loss: 21.55824851989746\n",
      "epoch: 116  train_loss: 27.196287155151367\n",
      "epoch: 117  train_loss: 21.15300941467285\n",
      "epoch: 118  train_loss: 21.00584602355957\n",
      "epoch: 119  train_loss: 25.931854248046875\n",
      "epoch: 120  train_loss: 22.11541175842285\n",
      "epoch: 121  train_loss: 23.250288009643555\n",
      "epoch: 122  train_loss: 22.796274185180664\n",
      "epoch: 123  train_loss: 20.92404556274414\n",
      "epoch: 124  train_loss: 27.458465576171875\n",
      "epoch: 125  train_loss: 20.886962890625\n",
      "epoch: 126  train_loss: 21.230859756469727\n",
      "epoch: 127  train_loss: 26.72294044494629\n",
      "epoch: 128  train_loss: 21.216358184814453\n",
      "epoch: 129  train_loss: 23.56781768798828\n",
      "epoch: 130  train_loss: 22.930261611938477\n",
      "epoch: 131  train_loss: 20.768381118774414\n",
      "epoch: 132  train_loss: 27.564056396484375\n",
      "epoch: 133  train_loss: 21.011751174926758\n",
      "epoch: 134  train_loss: 20.99507713317871\n",
      "epoch: 135  train_loss: 26.807174682617188\n",
      "epoch: 136  train_loss: 20.673187255859375\n",
      "epoch: 137  train_loss: 22.561782836914062\n",
      "epoch: 138  train_loss: 22.3968448638916\n",
      "epoch: 139  train_loss: 26.620939254760742\n",
      "epoch: 140  train_loss: 20.975788116455078\n",
      "epoch: 141  train_loss: 18.216001510620117\n",
      "epoch: 142  train_loss: 19.80831527709961\n",
      "epoch: 143  train_loss: 21.28532600402832\n",
      "epoch: 144  train_loss: 22.106063842773438\n",
      "epoch: 145  train_loss: 29.655885696411133\n",
      "epoch: 146  train_loss: 18.650423049926758\n",
      "epoch: 147  train_loss: 17.43042755126953\n",
      "epoch: 148  train_loss: 18.23661994934082\n",
      "epoch: 149  train_loss: 17.20149803161621\n",
      "epoch: 150  train_loss: 17.79774284362793\n",
      "epoch: 151  train_loss: 17.559885025024414\n",
      "epoch: 152  train_loss: 17.60611915588379\n",
      "epoch: 153  train_loss: 18.04026985168457\n",
      "epoch: 154  train_loss: 17.773319244384766\n",
      "epoch: 155  train_loss: 18.05536460876465\n",
      "epoch: 156  train_loss: 17.660263061523438\n",
      "epoch: 157  train_loss: 17.283323287963867\n",
      "epoch: 158  train_loss: 17.642343521118164\n",
      "epoch: 159  train_loss: 17.28803825378418\n",
      "epoch: 160  train_loss: 16.876848220825195\n",
      "epoch: 161  train_loss: 17.825963973999023\n",
      "epoch: 162  train_loss: 17.707860946655273\n",
      "epoch: 163  train_loss: 17.03038787841797\n",
      "epoch: 164  train_loss: 16.765783309936523\n",
      "epoch: 165  train_loss: 17.850645065307617\n",
      "epoch: 166  train_loss: 17.525894165039062\n",
      "epoch: 167  train_loss: 16.82121467590332\n",
      "epoch: 168  train_loss: 16.873470306396484\n",
      "epoch: 169  train_loss: 17.3770751953125\n",
      "epoch: 170  train_loss: 17.24763298034668\n",
      "epoch: 171  train_loss: 16.830886840820312\n",
      "epoch: 172  train_loss: 16.65178871154785\n",
      "epoch: 173  train_loss: 17.552154541015625\n",
      "epoch: 174  train_loss: 17.47084617614746\n",
      "epoch: 175  train_loss: 16.87845230102539\n",
      "epoch: 176  train_loss: 16.635272979736328\n",
      "epoch: 177  train_loss: 17.52560043334961\n",
      "epoch: 178  train_loss: 17.740102767944336\n",
      "epoch: 179  train_loss: 16.53154945373535\n",
      "epoch: 180  train_loss: 16.42306137084961\n",
      "epoch: 181  train_loss: 17.452173233032227\n",
      "epoch: 182  train_loss: 17.527212142944336\n",
      "epoch: 183  train_loss: 16.468063354492188\n",
      "epoch: 184  train_loss: 16.450998306274414\n",
      "epoch: 185  train_loss: 17.531641006469727\n",
      "epoch: 186  train_loss: 17.44577980041504\n",
      "epoch: 187  train_loss: 16.73783302307129\n",
      "epoch: 188  train_loss: 16.46605682373047\n",
      "epoch: 189  train_loss: 17.266067504882812\n",
      "epoch: 190  train_loss: 17.392353057861328\n",
      "epoch: 191  train_loss: 16.346677780151367\n",
      "epoch: 192  train_loss: 16.34113311767578\n",
      "epoch: 193  train_loss: 17.623071670532227\n",
      "epoch: 194  train_loss: 16.968870162963867\n",
      "epoch: 195  train_loss: 17.190919876098633\n",
      "epoch: 196  train_loss: 17.051191329956055\n",
      "epoch: 197  train_loss: 16.83064079284668\n",
      "epoch: 198  train_loss: 16.8945369720459\n",
      "epoch: 199  train_loss: 16.924461364746094\n",
      "epoch: 200  train_loss: 16.77057456970215\n",
      "epoch: 201  train_loss: 17.031187057495117\n",
      "epoch: 202  train_loss: 17.0504207611084\n",
      "epoch: 203  train_loss: 16.68132781982422\n",
      "epoch: 204  train_loss: 16.506406784057617\n",
      "epoch: 205  train_loss: 17.380722045898438\n",
      "epoch: 206  train_loss: 17.246265411376953\n",
      "epoch: 207  train_loss: 16.306299209594727\n",
      "epoch: 208  train_loss: 16.270448684692383\n",
      "epoch: 209  train_loss: 17.767845153808594\n",
      "epoch: 210  train_loss: 17.594812393188477\n",
      "epoch: 211  train_loss: 16.32640266418457\n",
      "epoch: 212  train_loss: 16.41316032409668\n",
      "epoch: 213  train_loss: 17.66057586669922\n",
      "epoch: 214  train_loss: 17.640764236450195\n",
      "epoch: 215  train_loss: 16.420459747314453\n",
      "epoch: 216  train_loss: 16.43659019470215\n",
      "epoch: 217  train_loss: 17.290695190429688\n",
      "epoch: 218  train_loss: 17.27776336669922\n",
      "epoch: 219  train_loss: 16.255491256713867\n",
      "epoch: 220  train_loss: 16.2888240814209\n",
      "epoch: 221  train_loss: 17.12770652770996\n",
      "epoch: 222  train_loss: 17.433225631713867\n",
      "epoch: 223  train_loss: 16.403213500976562\n",
      "epoch: 224  train_loss: 16.13132095336914\n",
      "epoch: 225  train_loss: 17.452899932861328\n",
      "epoch: 226  train_loss: 17.42942237854004\n",
      "epoch: 227  train_loss: 16.219505310058594\n",
      "epoch: 228  train_loss: 16.2584171295166\n",
      "epoch: 229  train_loss: 17.13536834716797\n",
      "epoch: 230  train_loss: 17.26955795288086\n",
      "epoch: 231  train_loss: 16.287675857543945\n",
      "epoch: 232  train_loss: 16.24540138244629\n",
      "epoch: 233  train_loss: 17.138071060180664\n",
      "epoch: 234  train_loss: 17.244787216186523\n",
      "epoch: 235  train_loss: 16.332975387573242\n",
      "epoch: 236  train_loss: 16.202425003051758\n",
      "epoch: 237  train_loss: 17.192123413085938\n",
      "epoch: 238  train_loss: 17.291839599609375\n",
      "epoch: 239  train_loss: 16.205039978027344\n",
      "epoch: 240  train_loss: 16.122777938842773\n",
      "epoch: 241  train_loss: 17.48335838317871\n",
      "epoch: 242  train_loss: 17.115034103393555\n",
      "epoch: 243  train_loss: 16.48847770690918\n",
      "epoch: 244  train_loss: 16.37884521484375\n",
      "epoch: 245  train_loss: 17.0538272857666\n",
      "epoch: 246  train_loss: 16.975784301757812\n",
      "epoch: 247  train_loss: 16.4792423248291\n",
      "epoch: 248  train_loss: 16.244863510131836\n",
      "epoch: 249  train_loss: 16.936548233032227\n",
      "epoch: 250  train_loss: 17.029436111450195\n",
      "epoch: 251  train_loss: 16.310121536254883\n",
      "epoch: 252  train_loss: 16.1849422454834\n",
      "epoch: 253  train_loss: 17.392791748046875\n",
      "epoch: 254  train_loss: 15.464489936828613\n",
      "epoch: 255  train_loss: 16.42137336730957\n",
      "epoch: 256  train_loss: 16.144102096557617\n",
      "epoch: 257  train_loss: 15.735950469970703\n",
      "epoch: 258  train_loss: 15.80937671661377\n",
      "epoch: 259  train_loss: 16.191652297973633\n",
      "epoch: 260  train_loss: 15.577812194824219\n",
      "epoch: 261  train_loss: 15.803268432617188\n",
      "epoch: 262  train_loss: 15.308460235595703\n",
      "epoch: 263  train_loss: 15.519389152526855\n",
      "epoch: 264  train_loss: 15.193421363830566\n",
      "epoch: 265  train_loss: 15.543110847473145\n",
      "epoch: 266  train_loss: 15.17192554473877\n",
      "epoch: 267  train_loss: 15.537110328674316\n",
      "epoch: 268  train_loss: 15.121578216552734\n",
      "epoch: 269  train_loss: 15.38290786743164\n",
      "epoch: 270  train_loss: 15.06302261352539\n",
      "epoch: 271  train_loss: 15.33809757232666\n",
      "epoch: 272  train_loss: 15.0278902053833\n",
      "epoch: 273  train_loss: 15.414749145507812\n",
      "epoch: 274  train_loss: 15.157227516174316\n",
      "epoch: 275  train_loss: 15.568026542663574\n",
      "epoch: 276  train_loss: 15.15998363494873\n",
      "epoch: 277  train_loss: 15.457261085510254\n",
      "epoch: 278  train_loss: 15.249455451965332\n",
      "epoch: 279  train_loss: 15.319343566894531\n",
      "epoch: 280  train_loss: 15.322779655456543\n",
      "epoch: 281  train_loss: 15.289694786071777\n",
      "epoch: 282  train_loss: 15.07336711883545\n",
      "epoch: 283  train_loss: 15.317146301269531\n",
      "epoch: 284  train_loss: 15.046874046325684\n",
      "epoch: 285  train_loss: 15.323330879211426\n",
      "epoch: 286  train_loss: 14.874064445495605\n",
      "epoch: 287  train_loss: 15.392139434814453\n",
      "epoch: 288  train_loss: 14.948758125305176\n",
      "epoch: 289  train_loss: 15.37104320526123\n",
      "epoch: 290  train_loss: 15.053421020507812\n",
      "epoch: 291  train_loss: 15.468964576721191\n",
      "epoch: 292  train_loss: 14.95610523223877\n",
      "epoch: 293  train_loss: 15.498344421386719\n",
      "epoch: 294  train_loss: 15.134407043457031\n",
      "epoch: 295  train_loss: 15.224047660827637\n",
      "epoch: 296  train_loss: 15.243639945983887\n",
      "epoch: 297  train_loss: 15.206021308898926\n",
      "epoch: 298  train_loss: 15.055179595947266\n",
      "epoch: 299  train_loss: 15.232081413269043\n",
      "epoch: 300  train_loss: 15.116242408752441\n",
      "epoch: 301  train_loss: 15.336593627929688\n",
      "epoch: 302  train_loss: 14.954363822937012\n",
      "epoch: 303  train_loss: 15.416037559509277\n",
      "epoch: 304  train_loss: 15.013104438781738\n",
      "epoch: 305  train_loss: 15.367076873779297\n",
      "epoch: 306  train_loss: 15.001908302307129\n",
      "epoch: 307  train_loss: 15.356404304504395\n",
      "epoch: 308  train_loss: 15.136425971984863\n",
      "epoch: 309  train_loss: 15.263413429260254\n",
      "epoch: 310  train_loss: 15.034127235412598\n",
      "epoch: 311  train_loss: 15.269068717956543\n",
      "epoch: 312  train_loss: 14.970301628112793\n",
      "epoch: 313  train_loss: 15.34247875213623\n",
      "epoch: 314  train_loss: 14.91348934173584\n",
      "epoch: 315  train_loss: 15.323623657226562\n",
      "epoch: 316  train_loss: 14.905661582946777\n",
      "epoch: 317  train_loss: 15.300017356872559\n",
      "epoch: 318  train_loss: 14.92409610748291\n",
      "epoch: 319  train_loss: 15.359283447265625\n",
      "epoch: 320  train_loss: 14.866446495056152\n",
      "epoch: 321  train_loss: 15.455225944519043\n",
      "epoch: 322  train_loss: 14.95098876953125\n",
      "epoch: 323  train_loss: 15.389007568359375\n",
      "epoch: 324  train_loss: 14.897453308105469\n",
      "epoch: 325  train_loss: 15.35488224029541\n",
      "epoch: 326  train_loss: 14.870661735534668\n",
      "epoch: 327  train_loss: 15.294848442077637\n",
      "epoch: 328  train_loss: 14.869601249694824\n",
      "epoch: 329  train_loss: 15.248763084411621\n",
      "epoch: 330  train_loss: 14.93027114868164\n",
      "epoch: 331  train_loss: 15.200518608093262\n",
      "epoch: 332  train_loss: 14.9436616897583\n",
      "epoch: 333  train_loss: 15.223750114440918\n",
      "epoch: 334  train_loss: 14.937952995300293\n",
      "epoch: 335  train_loss: 15.145076751708984\n",
      "epoch: 336  train_loss: 14.918201446533203\n",
      "epoch: 337  train_loss: 15.305500984191895\n",
      "epoch: 338  train_loss: 14.852108001708984\n",
      "epoch: 339  train_loss: 15.30878734588623\n",
      "epoch: 340  train_loss: 14.913268089294434\n",
      "epoch: 341  train_loss: 15.372845649719238\n",
      "epoch: 342  train_loss: 14.909123420715332\n",
      "epoch: 343  train_loss: 15.185798645019531\n",
      "epoch: 344  train_loss: 14.985417366027832\n",
      "epoch: 345  train_loss: 15.173797607421875\n",
      "epoch: 346  train_loss: 14.934898376464844\n",
      "epoch: 347  train_loss: 15.361157417297363\n",
      "epoch: 348  train_loss: 14.952171325683594\n",
      "epoch: 349  train_loss: 15.28396224975586\n",
      "epoch: 350  train_loss: 14.887286186218262\n",
      "epoch: 351  train_loss: 15.469426155090332\n",
      "epoch: 352  train_loss: 14.918540954589844\n",
      "epoch: 353  train_loss: 15.261832237243652\n",
      "epoch: 354  train_loss: 14.878727912902832\n",
      "epoch: 355  train_loss: 15.252220153808594\n",
      "epoch: 356  train_loss: 15.077288627624512\n",
      "epoch: 357  train_loss: 15.2891263961792\n",
      "epoch: 358  train_loss: 14.874838829040527\n",
      "epoch: 359  train_loss: 15.240412712097168\n",
      "epoch: 360  train_loss: 14.88805103302002\n",
      "epoch: 361  train_loss: 15.258894920349121\n",
      "epoch: 362  train_loss: 14.849648475646973\n",
      "epoch: 363  train_loss: 15.335609436035156\n",
      "epoch: 364  train_loss: 14.835503578186035\n",
      "epoch: 365  train_loss: 15.398402214050293\n",
      "epoch: 366  train_loss: 14.794116020202637\n",
      "epoch: 367  train_loss: 15.27951717376709\n",
      "epoch: 368  train_loss: 14.847115516662598\n",
      "epoch: 369  train_loss: 15.252306938171387\n",
      "epoch: 370  train_loss: 14.798548698425293\n",
      "epoch: 371  train_loss: 15.188728332519531\n",
      "epoch: 372  train_loss: 14.815438270568848\n",
      "epoch: 373  train_loss: 15.213493347167969\n",
      "epoch: 374  train_loss: 14.806900978088379\n",
      "epoch: 375  train_loss: 15.33492660522461\n",
      "epoch: 376  train_loss: 14.819539070129395\n",
      "epoch: 377  train_loss: 15.277968406677246\n",
      "epoch: 378  train_loss: 14.873435020446777\n",
      "epoch: 379  train_loss: 15.38794994354248\n",
      "epoch: 380  train_loss: 14.801335334777832\n",
      "epoch: 381  train_loss: 15.340339660644531\n",
      "epoch: 382  train_loss: 14.887972831726074\n",
      "epoch: 383  train_loss: 15.17431640625\n",
      "epoch: 384  train_loss: 14.885785102844238\n",
      "epoch: 385  train_loss: 15.271533012390137\n",
      "epoch: 386  train_loss: 14.807833671569824\n",
      "epoch: 387  train_loss: 15.203167915344238\n",
      "epoch: 388  train_loss: 14.762619018554688\n",
      "epoch: 389  train_loss: 15.226332664489746\n",
      "epoch: 390  train_loss: 14.82061767578125\n",
      "epoch: 391  train_loss: 15.166703224182129\n",
      "epoch: 392  train_loss: 14.940635681152344\n",
      "epoch: 393  train_loss: 15.269332885742188\n",
      "epoch: 394  train_loss: 14.891441345214844\n",
      "epoch: 395  train_loss: 15.3164644241333\n",
      "epoch: 396  train_loss: 14.818534851074219\n",
      "epoch: 397  train_loss: 15.364789009094238\n",
      "epoch: 398  train_loss: 14.768572807312012\n",
      "epoch: 399  train_loss: 15.11976146697998\n",
      "epoch: 400  train_loss: 14.90314769744873\n",
      "epoch: 401  train_loss: 15.316378593444824\n",
      "epoch: 402  train_loss: 14.828510284423828\n",
      "epoch: 403  train_loss: 15.33596134185791\n",
      "epoch: 404  train_loss: 14.922171592712402\n",
      "epoch: 405  train_loss: 15.26882266998291\n",
      "epoch: 406  train_loss: 14.984184265136719\n",
      "epoch: 407  train_loss: 15.18105697631836\n",
      "epoch: 408  train_loss: 15.054080963134766\n",
      "epoch: 409  train_loss: 15.067840576171875\n",
      "epoch: 410  train_loss: 15.068699836730957\n",
      "epoch: 411  train_loss: 15.012972831726074\n",
      "epoch: 412  train_loss: 14.895258903503418\n",
      "epoch: 413  train_loss: 15.105491638183594\n",
      "epoch: 414  train_loss: 14.855462074279785\n",
      "epoch: 415  train_loss: 15.132845878601074\n",
      "epoch: 416  train_loss: 14.770149230957031\n",
      "epoch: 417  train_loss: 15.218024253845215\n",
      "epoch: 418  train_loss: 14.782173156738281\n",
      "epoch: 419  train_loss: 15.181118965148926\n",
      "epoch: 420  train_loss: 14.778099060058594\n",
      "epoch: 421  train_loss: 15.4183349609375\n",
      "epoch: 422  train_loss: 14.88595199584961\n",
      "epoch: 423  train_loss: 15.203879356384277\n",
      "epoch: 424  train_loss: 15.003169059753418\n",
      "epoch: 425  train_loss: 15.070963859558105\n",
      "epoch: 426  train_loss: 14.879081726074219\n",
      "epoch: 427  train_loss: 15.180008888244629\n",
      "epoch: 428  train_loss: 14.764030456542969\n",
      "epoch: 429  train_loss: 15.14743423461914\n",
      "epoch: 430  train_loss: 14.8035249710083\n",
      "epoch: 431  train_loss: 15.239428520202637\n",
      "epoch: 432  train_loss: 14.852370262145996\n",
      "epoch: 433  train_loss: 15.179931640625\n",
      "epoch: 434  train_loss: 14.963711738586426\n",
      "epoch: 435  train_loss: 15.148943901062012\n",
      "epoch: 436  train_loss: 14.888346672058105\n",
      "epoch: 437  train_loss: 15.369819641113281\n",
      "epoch: 438  train_loss: 14.765724182128906\n",
      "epoch: 439  train_loss: 15.204667091369629\n",
      "epoch: 440  train_loss: 14.880730628967285\n",
      "epoch: 441  train_loss: 15.217667579650879\n",
      "epoch: 442  train_loss: 14.8933687210083\n",
      "epoch: 443  train_loss: 15.231220245361328\n",
      "epoch: 444  train_loss: 14.757232666015625\n",
      "epoch: 445  train_loss: 15.300494194030762\n",
      "epoch: 446  train_loss: 14.915573120117188\n",
      "epoch: 447  train_loss: 15.212573051452637\n",
      "epoch: 448  train_loss: 14.794112205505371\n",
      "epoch: 449  train_loss: 15.239197731018066\n",
      "epoch: 450  train_loss: 14.901766777038574\n",
      "epoch: 451  train_loss: 15.126068115234375\n",
      "epoch: 452  train_loss: 14.88766860961914\n",
      "epoch: 453  train_loss: 15.193706512451172\n",
      "epoch: 454  train_loss: 14.765542030334473\n",
      "epoch: 455  train_loss: 15.171585083007812\n",
      "epoch: 456  train_loss: 14.835362434387207\n",
      "epoch: 457  train_loss: 15.251429557800293\n",
      "epoch: 458  train_loss: 14.718795776367188\n",
      "epoch: 459  train_loss: 15.312919616699219\n",
      "epoch: 460  train_loss: 14.813850402832031\n",
      "epoch: 461  train_loss: 15.157188415527344\n",
      "epoch: 462  train_loss: 14.916656494140625\n",
      "epoch: 463  train_loss: 15.192779541015625\n",
      "epoch: 464  train_loss: 14.791922569274902\n",
      "epoch: 465  train_loss: 15.170271873474121\n",
      "epoch: 466  train_loss: 14.983254432678223\n",
      "epoch: 467  train_loss: 15.273223876953125\n",
      "epoch: 468  train_loss: 14.744468688964844\n",
      "epoch: 469  train_loss: 15.209075927734375\n",
      "epoch: 470  train_loss: 14.710883140563965\n",
      "epoch: 471  train_loss: 15.287781715393066\n",
      "epoch: 472  train_loss: 14.724480628967285\n",
      "epoch: 473  train_loss: 15.343832969665527\n",
      "epoch: 474  train_loss: 14.734444618225098\n",
      "epoch: 475  train_loss: 15.350449562072754\n",
      "epoch: 476  train_loss: 14.953543663024902\n",
      "epoch: 477  train_loss: 15.242867469787598\n",
      "epoch: 478  train_loss: 14.913344383239746\n",
      "epoch: 479  train_loss: 15.19703197479248\n",
      "epoch: 480  train_loss: 14.943317413330078\n",
      "epoch: 481  train_loss: 15.124847412109375\n",
      "epoch: 482  train_loss: 15.000561714172363\n",
      "epoch: 483  train_loss: 15.244656562805176\n",
      "epoch: 484  train_loss: 14.985304832458496\n",
      "epoch: 485  train_loss: 15.24781322479248\n",
      "epoch: 486  train_loss: 14.789413452148438\n",
      "epoch: 487  train_loss: 15.328404426574707\n",
      "epoch: 488  train_loss: 14.74492359161377\n",
      "epoch: 489  train_loss: 15.132733345031738\n",
      "epoch: 490  train_loss: 15.002823829650879\n",
      "epoch: 491  train_loss: 15.042675018310547\n",
      "epoch: 492  train_loss: 15.002388000488281\n",
      "epoch: 493  train_loss: 15.03081226348877\n",
      "epoch: 494  train_loss: 14.835494995117188\n",
      "epoch: 495  train_loss: 15.092414855957031\n",
      "epoch: 496  train_loss: 14.88403606414795\n",
      "epoch: 497  train_loss: 15.272445678710938\n",
      "epoch: 498  train_loss: 14.986689567565918\n",
      "epoch: 499  train_loss: 15.17523193359375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\n",
    "\"\"\"\n",
    "# # #  Dataset\n",
    "\"\"\"\n",
    "data_seed = 408\n",
    "np.random.seed(data_seed)\n",
    "batsize = 100\n",
    "n_dim = 20\n",
    "n_train = 300\n",
    "n_valid = 100\n",
    "n_test = 100\n",
    "\n",
    "budget = 10.0\n",
    "\n",
    "\n",
    "# create dictionaries with sampled datapoints with uniform distribution\n",
    "#data_loaded = np.load('portfolio_data/portfolio_var50_ineq50_eq1_ex12000.npz', allow_pickle=True)\n",
    "data_loaded = gen_portfolio_lto_data(n_dim,n_train,n_valid,n_test)\n",
    "Q_load = data_loaded['Q']\n",
    "A_load = data_loaded['A']\n",
    "G_load = data_loaded['G']\n",
    "h_load = data_loaded['h']\n",
    "x_load = data_loaded['x']\n",
    "p_train = data_loaded['trainX']\n",
    "p_valid = data_loaded['validX']\n",
    "p_test  = data_loaded['testX']\n",
    "sols_train = data_loaded['trainY']\n",
    "sols_valid = data_loaded['validY']\n",
    "sols_test  = data_loaded['testY']\n",
    "#feat_size_load = data_loaded['feat_size']\n",
    "\n",
    "print(\"p_train.dtype\")\n",
    "print( p_train.dtype )\n",
    "\n",
    "samples_train = {\"p\": torch.Tensor(p_train)}  # JK TODO fix this, reduced size for debugging\n",
    "samples_dev   = {\"p\": torch.Tensor(p_valid)}\n",
    "samples_test  = {\"p\": torch.Tensor(p_test )}\n",
    "\n",
    "# create named dictionary datasets\n",
    "train_data = DictDataset(samples_train, name='train')\n",
    "dev_data   = DictDataset(samples_dev,   name='dev')\n",
    "test_data  = DictDataset(samples_test,  name='test')\n",
    "# create torch dataloaders for the Trainer\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batsize, num_workers=0,\n",
    "                                            collate_fn=train_data.collate_fn, shuffle=True)\n",
    "dev_loader   = torch.utils.data.DataLoader(dev_data, batch_size=batsize, num_workers=0,\n",
    "                                            collate_fn=dev_data.collate_fn, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_data, batch_size=batsize, num_workers=0,\n",
    "                                            collate_fn=test_data.collate_fn, shuffle=True)\n",
    "# note: training quality will depend on the DataLoader parameters such as batch size and shuffle\n",
    "\n",
    "\n",
    "\"\"\n",
    "\n",
    "\"\"\"\n",
    "# # #  pQP primal solution map architecture\n",
    "\"\"\"\n",
    "# define neural architecture for the solution map\n",
    "func = blocks.MLP(insize=n_dim, outsize=n_dim,\n",
    "                bias=True,\n",
    "                linear_map=slim.maps['linear'],\n",
    "                nonlin=nn.ReLU,\n",
    "                hsizes=[n_dim*2] * 4)\n",
    "# define symbolic solution map with concatenated features (problem parameters)\n",
    "#xi = lambda p1, p2: torch.cat([p1, p2], dim=-1)\n",
    "#features = Node(xi, ['p1', 'p2'], ['xi'], name='features')\n",
    "sol_map = Node(func, ['p'], ['x'], name='map')\n",
    "# trainable components of the problem solution\n",
    "components = [sol_map]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# # # objective and constraints formulation in Neuromancer\n",
    "\"\"\"\n",
    "# variables\n",
    "x = variable(\"x\")\n",
    "\n",
    "# sampled parameters\n",
    "p = variable('p')\n",
    "Q = torch.Tensor(Q_load)\n",
    "\n",
    "# objective function\n",
    "lambd = 1.0\n",
    "f = torch.sum(-p*x, dim = 1) + torch.sum( x*torch.matmul(Q,x.T).T, dim=1 ) #-p@x + lambd * x@Q@x\n",
    "obj = f.minimize(weight=1.0, name='obj')\n",
    "objectives = [obj]\n",
    "\n",
    "# constraints\n",
    "e = torch.ones(n_dim)\n",
    "Q_con = 100.\n",
    "con_1 = Q_con*(torch.sum(x, dim=1) == budget) #Q_con*(e@x == 1)\n",
    "con_1.name = 'c1'\n",
    "con_2 = Q_con * (x >= 0)\n",
    "con_2.name = 'c2'\n",
    "\n",
    "constraints = [con_1, con_2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# # #  problem formulation in Neuromancer\n",
    "\"\"\"\n",
    "# create penalty method loss function\n",
    "loss = PenaltyLoss(objectives, constraints)\n",
    "# construct constrained optimization problem\n",
    "problem = Problem(components, loss)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# # #  problem solution in Neuromancer\n",
    "\"\"\"\n",
    "optimizer = torch.optim.AdamW(problem.parameters(), lr=1e-3)\n",
    "# define trainer\n",
    "trainer = Trainer(\n",
    "    problem,\n",
    "    train_loader,\n",
    "    dev_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    epochs=500,\n",
    "    patience=100,\n",
    "    warmup=100,\n",
    "    train_metric=\"train_loss\",\n",
    "    dev_metric=\"dev_loss\",\n",
    "    test_metric=\"test_loss\",\n",
    "    eval_metric=\"dev_loss\",\n",
    ")\n",
    "\n",
    "# Train solution map\n",
    "best_model = trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This Cell Computes a Heuristic Metric Choice For ADMM For this QP problem\n",
    "following\n",
    "\n",
    "\n",
    "https://web.stanford.edu/~boyd/papers/pdf/metric_select_DR_ADMM.pdf\n",
    "https://web.stanford.edu/~boyd/papers/pdf/metric_select_fdfbs.pdf\n",
    "\n",
    "'''\n",
    "\n",
    "# Import packages.\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "from scipy.linalg import ldl,eigh,eigh_tridiagonal, null_space\n",
    "\n",
    "#We start here by defining our QP problem\n",
    "#Objective\n",
    "Qm = Q.detach().numpy().copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# DEFINE THE CONSTRAINTS\n",
    "# sum(x) = 1\n",
    "# x>0\n",
    "\n",
    "LX_eq = np.ones((1,n_dim))\n",
    "LX_ineq = np.eye(n_dim)\n",
    "\n",
    "LX = np.concatenate((LX_eq,LX_ineq),axis = 0)\n",
    "LS = np.concatenate( (np.zeros((1,n_dim)),np.eye(n_dim) ) ,axis = 0)\n",
    "L = np.concatenate((LX,LS),axis=1)\n",
    "\n",
    "\n",
    "# LX = np.array([[-1, -1],\n",
    "#      [1, 1],\n",
    "#      [1,-1],\n",
    "#      [-1,1]])\n",
    "# LS = np.eye(4)\n",
    "# L = np.concatenate((LX,LS),axis=1)\n",
    "\n",
    "### Find the Hessian of d1\n",
    "#define objective including slack vars\n",
    "QXS = np.concatenate((Q,np.zeros((n_dim,n_dim))),axis = 1)\n",
    "QXS = np.concatenate((QXS,np.zeros((n_dim,2*n_dim))),axis = 0)\n",
    "\n",
    "QL = np.concatenate((QXS,np.transpose(L)),axis = 1) \n",
    "LO = np.concatenate((L,np.zeros((L.shape[0],L.shape[0]))),axis = 1)\n",
    "QL = np.concatenate((QL,LO),axis=0)\n",
    "\n",
    "P = np.linalg.inv(QL)\n",
    "P11 = P[0:2*n_dim,0:2*n_dim]\n",
    "e, V = eigh(P11)\n",
    "R_T = np.matmul(V,np.diag(np.sqrt(np.maximum(e,0))))\n",
    "R = np.transpose(R_T)\n",
    "# then we have R_T R = P11\n",
    "\n",
    "Id = np.eye(2*n_dim)\n",
    "M = cp.Variable((2*n_dim,2*n_dim),symmetric = True)\n",
    "t = cp.Variable()\n",
    "obj = cp.Minimize(-t)\n",
    "\n",
    "constraints = [M >>0]\n",
    "constraints += [ R@M@R_T << Id]\n",
    "constraints += [ R@M@R_T >> t*Id]\n",
    "prob = cp.Problem(obj,constraints)\n",
    "prob.solve()\n",
    "\n",
    "Pm = M.value\n",
    "\n",
    "import scipy.linalg as lg\n",
    "U,s,V = lg.svd(Pm)\n",
    "E = np.matmul(U,np.matmul(np.diag(np.sqrt(s)),np.transpose(U)))\n",
    "\n",
    "\n",
    "#compute gamma heuristic\n",
    "ye, yV = eigh(np.matmul(E,np.matmul(P11,E)))\n",
    "zero_thresh = 1e-10 # will consider anything below this threshold zero\n",
    "ye_notZero = ye[ye>zero_thresh]\n",
    "gamma = 1/(np.sqrt(np.max(ye_notZero)*np.min(ye_notZero)))\n",
    "\n",
    "\n",
    "\n",
    "### Check that Q is strongly convex on Null Space of L\n",
    "NL = null_space(L)\n",
    "Q_NL = np.matmul(np.transpose(NL),np.matmul(QXS,NL)) \n",
    "qnl_eigs = eigh(Q_NL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "#######################################\n",
    "#######################################\n",
    "\n",
    "Add on ADMM correction layer\n",
    "\n",
    "#######################################\n",
    "########################################\n",
    "'''\n",
    "\n",
    "\n",
    "import ADMM as am\n",
    "from importlib import reload\n",
    "\n",
    "reload(am)\n",
    "\n",
    "\n",
    "'''\n",
    "# DEFINE THE OBJECTIVE\n",
    "'''\n",
    "# x is assumed to include slack variables!\n",
    "def f_obj(x,p):\n",
    "    x = x[:n_dim]\n",
    "    return -p@x + lambd * x@(Q@x)            #lambd * torch.sum(x*torch.mv(Q,x))   #torch.sum( -p*x  ) +          #-p@x + lambd * x@(Q@x)\n",
    "\n",
    "\n",
    "'''\n",
    "# DEFINE THE CONSTRAINTS\n",
    "'''\n",
    "def F_ineq(x,p):\n",
    "    x = x[:n_dim]\n",
    "    return -x\n",
    "\n",
    "def F_eq(x,p):\n",
    "    x = x[:n_dim]\n",
    "    return (x.sum() - budget).unsqueeze(0)\n",
    "\n",
    "\n",
    "num_steps = 10 # number of ADMM iterations to take\n",
    "x_dim = n_dim # dimension of primal variable\n",
    "n_ineq = n_dim #number of inequality constraints\n",
    "n_eq = 1\n",
    "parm_dim = n_dim #number of problem parameters\n",
    "#E = np.eye(2*n_dim)\n",
    "opt_met = am.OptMetric(x_dim,parm_dim,E)\n",
    "\n",
    "solver = am.ADMMSolver(\n",
    "    f_obj = f_obj, \n",
    "    F_ineq = F_ineq,\n",
    "    F_eq = F_eq,\n",
    "    x_dim = x_dim, \n",
    "    n_ineq = n_ineq,\n",
    "    n_eq = n_eq, \n",
    "    JF_fixed=True,\n",
    "    parm_dim = parm_dim,\n",
    "    num_steps = num_steps,\n",
    "    Metric = opt_met,\n",
    "    gamma = gamma\n",
    "    )\n",
    "\n",
    "\n",
    "# REMAP THROUGH CORRECTION\n",
    "sol_map = Node(func, ['p'], ['x_predicted'], name='map')\n",
    "ADMM_correction = Node(solver,['x_predicted','p'],['x'])\n",
    "components = [sol_map, ADMM_correction]\n",
    "\n",
    "objectives = []\n",
    "constraints = []\n",
    "# create loss function\n",
    "loss = PenaltyLoss(objectives, constraints)\n",
    "# construct constrained optimization problem\n",
    "problem = Problem(components, loss)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.num_steps =200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples_test['p']\n",
      "tensor([[ 0.0786, -0.1396,  0.0544,  ..., -0.0198,  0.0820, -0.1486],\n",
      "        [ 0.1672, -0.0667,  0.1019,  ..., -0.0181,  0.4626, -0.1047],\n",
      "        [-0.3325, -0.2108, -0.3959,  ...,  0.1514, -0.1867, -0.1235],\n",
      "        ...,\n",
      "        [ 0.1383, -0.1055,  0.1441,  ...,  0.0018,  0.0623,  0.3851],\n",
      "        [-0.1949, -0.4182,  0.0315,  ..., -0.4287,  0.2174,  0.0565],\n",
      "        [-0.2034, -0.2070,  0.5283,  ..., -0.1636, -0.1470,  0.3831]])\n",
      "cv/nm time: 0.06430247816959102\n",
      "Average Solution Difference: 3.128576e-07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t = time.time()\n",
    "with torch.no_grad():\n",
    "    samples_test['name'] = 'test'\n",
    "    model_out = problem(samples_test)\n",
    "nm_time = time.time() - t\n",
    "\n",
    "\n",
    "x_nm_test = model_out['test_' + \"x\"].detach().numpy()\n",
    "x_loaded_test  = data_loaded['testY']\n",
    "\n",
    "print(\"samples_test['p']\")\n",
    "print(samples_test['p'])\n",
    "\n",
    "cvxpy_layer = cvx_qp(n_dim,Q,budget = budget)\n",
    "t = time.time()\n",
    "x_cvxpy_test = cvxpy_layer(samples_test['p'])\n",
    "cv_time = time.time() -t\n",
    "\n",
    "# print(\"x_nm_test\")\n",
    "# print( x_nm_test )\n",
    "# print(\"x_loaded_test\")\n",
    "# print( x_loaded_test )\n",
    "# print(\"x_cvxpy_test\")\n",
    "# print( x_cvxpy_test )\n",
    "\n",
    "\n",
    "print('cv/nm time:',cv_time/nm_time)\n",
    "\n",
    "sol_diff = np.mean(np.sum((x_cvxpy_test.detach().numpy() - x_nm_test)**2,axis=-1))\n",
    "print(\"Average Solution Difference:\",np.mean(sol_diff))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# '''\n",
    "# Compute Train set error\n",
    "# '''\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     t = time.time()\n",
    "#     samples_train['name'] = 'train'\n",
    "#     model_out = problem(samples_train)\n",
    "#     nm_time = time.time() - t\n",
    "\n",
    "\n",
    "# x_nm_test = model_out['train_' + \"x\"].detach().numpy()\n",
    "# x_loaded_test  = data_loaded['trainY']\n",
    "\n",
    "\n",
    "# cvxpy_layer = cvx_qp(n_dim,Q)\n",
    "# x_cvxpy_test = cvxpy_layer(samples_train['p'])\n",
    "\n",
    "# # print(\"x_nm_test\")\n",
    "# # print( x_nm_test )\n",
    "# # print(\"x_loaded_test\")\n",
    "# # print( x_loaded_test )\n",
    "# # print(\"x_cvxpy_test\")\n",
    "# # print( x_cvxpy_test )\n",
    "\n",
    "# sol_diff = np.mean(np.sum((x_cvxpy_test.detach().numpy() - x_nm_test)**2,axis=-1))\n",
    "# print(\"Average Solution Difference:\",np.mean(sol_diff))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convergence Evaluation\n",
    "'''\n",
    "solver.num_steps = 40\n",
    "\n",
    "test_p = torch.tensor([ 0.0786, -0.1396,  0.0544,  0.5873, -0.1937,  0.0593,  0.2089, -0.2672,\n",
    "         0.2413,  0.1833, -0.1695,  0.1558,  0.3340,  0.2615, -0.0777, -0.2420,\n",
    "        -0.4748, -0.0198,  0.0820, -0.1486],dtype = torch.float32)\n",
    "\n",
    "test_x = torch.tensor([0.0571, 0.0828, 0.1068, 0.0153, 0.0139, 0.0655, 0.0151, 0.0279, 0.0574,\n",
    "        0.0053, 0.0264, 0.0175, 0.0355, 0.0876, 0.0247, 0.0498, 0.0314, 0.0840,\n",
    "        0.0746, 0.1085],dtype = torch.float32)\n",
    "\n",
    "\n",
    "#test_p = torch.tensor([p1,p2],dtype=torch.float32)\n",
    "#test_x = torch.tensor([0.32,-0.37],dtype=torch.float32)\n",
    "cvxpy_layer = cvx_qp(n_dim,Q,budget)\n",
    "x_cvxpy_test = cvxpy_layer(test_p)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_x = torch.unsqueeze(test_x,0)\n",
    "test_p = torch.unsqueeze(test_p,0)\n",
    "\n",
    "x_hist = solver(test_x,test_p)[3]\n",
    "x_hist = torch.stack(x_hist).detach().numpy()\n",
    "x_hist = x_hist[:,0,0:20]\n",
    "\n",
    "#np.save('Convergence_Data/portfolio20_ADMM_Wheuristic.npy',x_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x306710280>]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.log10(np.sum((x_hist - x_cvxpy_test.detach().numpy())**2,axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuromancer_functorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
